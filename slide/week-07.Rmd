---
title: "SSPS [4102|6006] Data Analytics</br>[in the Social Sciences|for Social Research]"
subtitle: "Week 07</br>Probability"
author: "Francesco Bailo"
date: "Semester 2, 2024 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '4:3' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling

---

background-image: url('assets/USydLogo.svg')
background-size: 95%

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

```{r setup, include=FALSE}

options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      dev = 'svg', 
                      fig.width = 4, 
                      fig.height = 4, out.width="30%",
                      fig.align="center")

library(knitr)
library(kableExtra)
library(tidyverse)
library(sf)
library(DiagrammeR)
library(cowplot)
library(gapminder)

ggplot2::theme_set(theme_bw())

```

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.

---
class: inverse, center, middle

# Probability 

---

## Recap from last week

- `lm()` with one independent variable.
- $R^2$, a model fit measure, which represents the proportion of variability in $Y$ (outcome or dependent variable) that is explained by a linear combination of $X$ (predictors or independent variables).  

---
class: inverse, center, middle

# But ... Before talking about probability, let's go back a bit and talk about multiple regression models

---

## Multiple Linear Regression Models

Linear models with more than one $X$ variable

$$\widehat{Y}_i = \widehat{\alpha} + \widehat{\beta}_1 X_{i1}+... + \widehat{\beta}_p X_{ip}$$

where:

- $\widehat{Y_i}$ is the predicted value of $Y$ for observation $i$

- $\widehat{\alpha}$ is the estimated intercept coefficient  

 each $\widehat{\beta}_j$ (pronounced beta hat sub j) is the estimated coefficient for variable $X_j$ ( $j{=} {1}, ..., p$ ) - we use $j$ as a stand-in for all different subscripts.

- each $X_{ij}$ is the observed value of the variable $X_j$ for observation $i$ ( $j{=} {1}, ..., p$ )

- $p$ is the total number of $X$ variables in the model.

.footnote[Slide adapted from slides by Prof. Llaudet]


---

## Simple vs Multiple regression

.pull-left[

| simple regression                          |
|--------------------------------------------|
| $\widehat{Y} = \widehat{\alpha} + \widehat{\beta} X$  |
| $\widehat{\alpha}$: $\widehat{Y}$ when $X=0$ |
| $\widehat{\beta}$: $\triangle\widehat{Y}$ associated with $\triangle X=1$ |


]

.pull-right[

| multiple regression                                                             |
|---------------------------------------------------------------------------------|
| $\widehat{Y} = \widehat{\alpha} + \widehat{\beta}_{1} X_{1} + ... + \widehat{\beta}_p X_{p}$  |
| $\widehat{\alpha}$: $\widehat{Y}$ when all $X_j=0$ ( $j=1,...,p$ ) |
| each $\widehat{\beta}_j$: $\triangle\widehat{Y}$ associated with $\triangle X_j=1$, while holding all other $X$ variables constant or *ceteris paribus* |


]

.footnote[Slide adapted from slides by Prof. Llaudet]

---

## Interpretation of Coefficients in Multiple Linear Regression Models

$$\widehat{Y} = \widehat{\alpha} + \widehat{\beta}_{1} X_{1} + ... + \widehat{\beta}_p X_{p}$$

- $\widehat{\alpha}$ is the $\widehat{Y}$ when *all* $X_j{=}{0}$

- Because there are multiple $X$ variables, there are multiple $\widehat{\beta}$ coefficients (one for each $X$ variable)

.content-box-yellow[

- Each $\widehat{\beta}_j$ is the $\triangle \widehat{Y}$ associated with $\triangle X_j$=1, *while holding all other $X$ variables constant* 

]

.footnote[Slide adapted from slides by Prof. Llaudet]
---

## Interpretation of $\widehat{\beta_1}$ when $X_1$ is the treatment variable and the other X variables are all the potential confounding variables

- Adding all confounders as controls in the model makes treatment and control groups comparable *after controls*

- As a result, we can interpret $\widehat{\beta}_1$ using **causal langauge**

+ $\widehat{\beta}_1$ is the $\triangle \widehat{Y}$ *caused by* the presence of the treatment ( $\triangle X_1$=1 ), while holding all confounders constant 

- $\widehat{\beta}_1$ should be a valid estimate of the average treatment effect if all confounding variables  are in the model


.footnote[Slide adapted from slides by Prof. Llaudet]

---
class: inverse, center, middle

# Lab: lm() with multiple regression

---

## Example: US midterm elections<sup>1</sup>

| Variable name | Description |
| ------------- | ----------- |
| `year` | midterm election year |
| `president` | name of president |
| `party` | Democrat or Republican |
| `approval` | Gallup approval rating at midterms |
| `seat.change` | change in the number of House seat's for the president's party |
| `rdi.change` | change in real disposable income over the year before |


.footnote[[1] Slides adapted from http://www.mattblackwell.org/files/teaching/gov50/regression-ii.pdf]


---

```{r echo = F}
midterms <- read.csv("../data/midterms.csv")
```

```{r eval = F}
midterms <- read.csv("midterms.csv")
```


```{r echo = F}
midterms %>%
  slice(1:5) %>%
  kable()
```

---

## Simple regression

$$ \widehat{seat.change} = \hat{\alpha} + \hat{\beta} \times approval $$

```{r}
fit_simple <- 
  lm(formula = seat.change ~ approval, 
     data = midterms)
```

.content-box-yellow[

Remember: the attribute `formula` is constructed as `outcome ~ predictor`.

]

---

```{r}
summary(fit_simple)
```

So far, we discussed "Estimate", which is the coefficient for each independent variable ( $\hat{\beta}_j$) and the intercept ( $\hat{\alpha}$ ) and the "R-squared", which is the proportion of the variation in $Y$ explained by the model.

---

## Mutliple regression

Using the `formula` syntax `outcome ~ predictor_1 + predictor_2 + ...` we can add additional predictors (i.e independent variables).

```{r}
fit_multi <- 
  lm(formula = seat.change ~ approval + rdi.change, 
     data = midterms)
```

---

<div style = 'font-size: 14pt'>

```{r}
summary(fit_multi)
```

</div>
The only difference here is that we have an extra row for the additional predictors. The interpretation of the "Estimate" now is variation in $\hat{Y}$ ( i.e. $\Delta\hat{Y}$ ) when variation in $X_j$ is 1 ( i.e. $\Delta X_j=1$ ).

---

## Using predict()

If we don't pass new data, the function `predict()` will just return the prediction for $Y$ (i.e. $\hat{Y}$ ) for each observation in our original data.

```{r}
predict(fit_multi)
```

The number you see above the prediction is the row number of you `midterms` (which is replaced by labels if your rows are named). Note that the first row is missing because one predictor value is missing...  

```{r}
midterms[1,]
```

---

## Predict based on you data

But we don't need to use the original data for our predictions. Remember that `lm()` gives us a continuous straight line; we can predict $Y$ for any value of each $X_j$. 

Let's get $\widehat{seat.change}$ using our simple model for an approval rate of 20% and 80% (which is not in the original data set).

```{r}
my_new_data <- #<<
  data.frame(approval = c(20, 80)) #<<

predict(fit_simple, 
        newdata = my_new_data) #<<
```

With an approval rate of 20 the president's party is predicted to lose 68 seats and with an approval rate of 80 to gain 17 seats.

---

## How confident are we in our prediction?

To obtain the 95% confidence intervals (more on this next week) around our predicted values ( $\hat{Y}$ ), we can add `interval = "confidence"`.

```{r}
seat.change_prediction <- # Storing prediction #<< 
  predict(fit_simple, 
          newdata = my_new_data, 
          interval = "confidence") # Adding confidence interval #<<
seat.change_prediction 
```

---

# Which we can then plot... 

```{r out.width = "30%"}
# First we need to create a data.frame adding the `approval` rate
# that we want to use as input values
data.frame(seat.change_prediction,
           approval = c(20, 80)) |>
# Then we plot  
  ggplot(aes(y = fit, 
             ymin = lwr, ymax = upr, 
             x = approval)) +
  geom_point() +
  geom_errorbar() +
  labs(y = "seat.change")
```

---

## Let's compare our predictions with the actual results ... 

- We are in 2018, just before the mid-term. (Trump is president).

- We want to use `seat.change` to predict the outcome of the upcoming midterm election for Trump's party. 

- Note that the 2018-Trump row was not used to estimate the model because of the missing value (of course, we are before the mid-term).

- We only know that Trump's approval rate in 2018 was 38%. So we can plug that value in...

```{r}
predict(fit_simple, newdata = data.frame(approval = 38))
```

We predict his party will lose about **43 seats**.

---

## Does adding predictors improve the accuracy of our model?

This is out multivariate regression model: in addition to `approval` we are also using `rdi.change` (change in real disposable income over the year before, an economic measure)

```{r}
fit_multi
```

---

```{r}
predict(fit_multi, 
        newdata = data.frame(approval = 38,
                             rdi.change = 4.1)) # Observed value #<<
```

Using the multiple regression model, we predict his party will lose **47 seats** instead of about **43 seats** with the simple regression model.

## Which model did better?

### Let's check Wikipedia!

---

<iframe id="wiki" src="https://en.wikipedia.org/wiki/2018_United_States_elections" width = '900px' height = '600px'></iframe>

---

.content-box-yellow[

Adding extra an variable in this case **decreased** the accuracy of the model!

]

.content-box-red[

The substantial interpretation here is that the national economy does not explain voting behaviour as much as we think...

]



### How could we know?

We could have checked the "Adjusted R-squared" of the two models...

The "Multiple R-squared", the $R^2$ we saw last week, explains the variation in the outcome variable that is explained by the predictors. Yet, the multiple R-squared will always increase if we add new predictors.

This is way it is more appropriate to compare the "Adjusted R-squared" when we add new predictors. 

---

```{r}
summary(fit_simple)
```

---

```{r}
summary(fit_multi)
```

---
class: inverse, center, middle

# Individual in-class quiz/tutorial (all)

---
class: inverse, center, middle

# Group in-class problem set

---
class: inverse, center, middle

# Attendance

---
class: inverse, center, middle

# See you next week with Uncertainty!

