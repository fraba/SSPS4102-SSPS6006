---
title: "SSPS [4102|6006] Data Analytics</br>[in the Social Sciences|for Social Research]"
subtitle: "Week 11</br>Textual Data: Machine Learning Methods"
author: "Francesco Bailo"
date: "Semester 2, 2024 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '4:3' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling

---

background-image: url('assets/USydLogo.svg')
background-size: 95%

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

```{r setup, include=FALSE}

options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = FALSE, 
                      dev = 'svg', 
                      fig.width = 4, 
                      fig.height = 4, out.width="30%",
                      fig.align="center")

library(knitr)
library(kableExtra)
library(tidyverse)
library(sf)
library(DiagrammeR)
library(cowplot)
library(gapminder)
library(ggrepel)

ggplot2::theme_set(theme_bw())

```

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.
---
class: inverse, center, middle

# Please take the anonymous Week 11 Student feedback survey on Canvas!

```{r echo = FALSE}
library(countdown)

countdown(minutes = 5, seconds = 00)
```

---

## Recap from last week

- Language and Modelling
- Tokenization, Stop Words & Stemming
- Sentiment analysis
- Document-Frequency Matrix
- Word Embeddings

---

## Today's class

- tidymodels for machine learning
- Regression with non-text data
- Classification with non-text data
- Regression with text data
- Classification with text data

---

## Plan for today

| Time        | Content                                |
|-------------|----------------------------------------|
| 11:00-11:10 | Week 11 Student feedback survey        |
| 11:10-12:00 | A3 Workshop                            |
| 12:00-12:20 | Tidymodels for machine learning        |
| 12:20-12:40 | Logistic regression with non-text data |
| 12:40-12:50 | Linear regression with non-text data   |
| 12:50-13:00 | Individual quiz/tutorial Part 1        |
| 13:00-13:20 | Linear regression with text data       |
| 13:20-13:40 | Logistic regression with text data     |
| 13:40-14:00 | Individual quiz/tutorial Part 2        |

---

class: inverse, center, middle

# Week 11 Student feedback survey (from Canvas)

```{r echo = FALSE}
library(countdown)
countdown(minutes = 5, seconds = 00)
```

---

class: inverse, center, middle

# Individual on A3 with peer-review

```{r echo = FALSE}
library(countdown)

countdown(minutes = 30, seconds = 00)
```


---
class: inverse, center, middle

# Tidymodels for machine learning

---

## What is machine learning again?

.center[<img src = 'https://i.vas3k.blog/7w1.jpg' width = '100%'></img>]

.footnote[Illustration from: https://vas3k.com/blog/machine_learning/]

---

## A machine learning pipline

```{r echo = FALSE, out.width='100%'}
# Load the DiagrammeR package
library(DiagrammeR)

DiagrammeR::grViz("digraph {
  # Define graph properties for a more spaced layout
  graph [layout = dot, rankdir = TB]
  
  # Define nodes with increased font size
  node [shape = ellipse, style = filled, fillcolor = lightblue, fontsize=55]
  raw_data     [label = 'Raw Data']
  train_data   [label = 'Training Data']
  test_data    [label = 'Test Data']
  
  node [shape = ellipse, style = filled, fillcolor = green, fontsize=55]
  preprocess_train [label = 'Preprocess\nTraining Data']
  preprocess_test  [label = 'Preprocess\nTest Data']
  
  node [shape = ellipse, style = filled, fillcolor = orange, fontsize=55]
  model_spec      [label = 'Model Specification']
  model_training  [label = 'Model Training']
  
  node [shape = ellipse, style = filled, fillcolor = pink, fontsize=55]
  trained_model   [label = 'Trained Model']
  predict         [label = 'Predict on Test Data']
  evaluate        [label = 'Evaluate Predictions']
  
  # Define edges to create space between steps
  raw_data -> train_data [minlen=2]
  raw_data -> test_data [minlen=2]
  
  train_data -> preprocess_train [minlen=2]
  preprocess_train -> model_spec [minlen=2]
  model_spec -> model_training [minlen=2]
  model_training -> trained_model [minlen=2]
  
  test_data -> preprocess_test [minlen=2]
  preprocess_test -> predict [minlen=2]
  predict -> evaluate [minlen=2]
  trained_model -> predict [minlen=2]
  }",
  height = 500)
```


---

## What is tidymodels?

<iframe src = 'https://www.tidymodels.org/' width = '100%' height = '450px'></iframe>

.footnote[https://www.tidymodels.org/]

---
## Benefit of tidymodels

- Consistent and modular framework for modelling.
- Easier to use in a pipeline format.
- Compatible with a wide range of data preprocessing and validation methods.

.center[<img src = 'https://rviews.rstudio.com/post/2019-06-14-a-gentle-intro-to-tidymodels_files/figure-html/tidymodels.png' width = '100%'></img>]


---

## Steps in Tidymodels 

1.  Split Data (`initial_split()`)
2.  Define Model Specification
3.  Preprocess Data with Recipe (`recipe()`)
4.  Create Workflow (`workflow()`): 2. Specification + 3. Recipe
5.  Evaluate Model Performance


---

class: inverse, center, middle

# Logistic regression

---

## 1. Loading and preparing the data

```{r}
library(tidyverse)
library(tidymodels)
voting <- read.csv("../data/voting.csv")
head(voting)
```

Let's first recode the `voted` variable to a factor for the classification (no missing values here...)

```{r}
voting <- 
  voting |>
  dplyr::mutate(voted = 
                  factor(case_when(voted ==  0 ~ "no", # first case
                                   voted == 1 ~ "yes"))) # second case
```

---

## 2. Splitting the data

```{r}
nrow(voting)

set.seed(2006)
voting_split <- initial_split(voting, prop = .5)
```

```{r}
voting_train <- training(voting_split)
nrow(voting_train)
voting_test <- testing(voting_split)
nrow(voting_split)
```


---

## Preliminary data analysis (Know Your Data!)

.pull-left[

### Our treatment variable (independent)

```{r out.width = "70%"}
voting_train %>% 
  ggplot(aes(x = message)) +
  geom_bar()
```

]



.pull-right[

### Our outcome variable (dependent)

```{r out.width = "70%"}
voting_train %>% 
  ggplot(aes(x = voted)) +
  geom_bar()
```

]

---

## Preliminary data analysis (Know Your Data!)

### Our control variable

```{r out.width = "70%", fig.width=6, fig.height=3}
voting_train %>% 
  ggplot(aes(x = birth)) +
  geom_bar()
```

.center[(And this is where the boomers come from!)]

---

## 3. Define Model Specification

Now it is time to specify the model. Note that we are not using any data at this stage. Indeed, this specification can be used for different datasets.

1. We specify that we want a logistic regression, 
2. that we use the package ("engine") .content-box-purple[**glm**], which we already know, and finally  
3. that we want to do a classification (ML is either about .content-box-purple[**regression**] or .content-box-purple[**classification**])

```{r}
logistic_spec <- 
  logistic_reg() |> #<<
  set_engine("glm") |> #<<
  set_mode("classification") #<<
```

---

## 4. Define Model Recipe (including our formula)

The recipe is mainly our formula and the data, both in the format we should already be familiar with

```{r}
logistic_recipe <- 
  recipe(voted ~ message + birth, data = voting_train) #<<
```

(Note additional steps, such as normalisation and the creation of dummy variables are possible at this stage)


---

## 5. Create a Workflow

We add our .content-box-yellow[**Specification**] and .content-box-yellow[**Recipe**]

```{r}
logistic_workflow <- 
  workflow() |> #<<
  add_model(logistic_spec) |> #<<
  add_recipe(logistic_recipe) #<<
```

## 6. Train the Model

And then we train our model (with the training data)

```{r}
logistic_fit <- 
  logistic_workflow |> #<<
  fit(data = voting_train) #<<
```

---

## Check the parameters (a.k.a. estimates or coefficients) of the model

... and since this is a logistic regression you can also check the p-value.

To extract these values we can use `tidy()`.

```{r}
tidy(logistic_fit)
```

But with ML with are also interested in the **prediction**!

---

## 7. Evaluate Model Performance 

... using the .content-box-yellow[**test**] data 

```{r}
logistic_predictions <- 
  logistic_fit |>
  predict(voting_test, type = "prob") # This will return the probabilities
head(logistic_predictions)
```

.content-box-green[

With a .5 threshold I will predict a 'no' to `voted` if the value of `.pred_no` is >0.5.

]

---

## 7. Evaluate Model Performance 

And without `type = "prob"` we get the prediction (with a  0.5 decision treshold)

```{r}
logistic_predictions <- 
  logistic_fit |>
  predict(voting_test) # This will return the prediction
head(logistic_predictions)
```


---

## 7. Evaluate Model Performance

But how good is my model in predicting the actual observed value? 

I can bind the columns from the original test data and compare the actual observed value (`voted`) with the prediction.

```{r}
logistic_predictions <- 
  logistic_predictions |>
  dplyr::bind_cols(voting_test)
head(logistic_predictions)
```

---

## 7. Evaluate Model Performance

Let's check how many predictions are right using `accuracy()`

- Accuracy

```{r}
accuracy(logistic_predictions, truth = voted, estimate = .pred_class)
```


And finally here is our confusion matrix (which identify a big problem!)

```{r}
conf_mat(logistic_predictions, truth = voted, estimate = .pred_class)
```

---

## 7. Evaluate Model Performance

In conclusion, only knowing `message` and `birth` (age) we can predict if someone will vote or not with an accuracy of 69%. 

> The model will do an OK good job in predicting the NOs but will do a terrible job in predicting the YESs.

---

class: inverse, center, middle

# Linear regression

---

### 1. Loading and preparing the data

```{r}
midterms <- 
  read.csv("../data/midterms.csv") |>
  tidyr::drop_na() #<< # This will drop NAs
head(midterms)
```

### 2. Splitting the data

```{r}
set.seed(2006)
midterms_split <- initial_split(midterms, prop = .5)
midterms_train <- training(midterms_split)
nrow(midterms_train)
midterms_test <- testing(midterms_split)
nrow(midterms_train)
```

---

## 3. Define Model Specification

```{r}
linear_spec <- 
  linear_reg() %>%
  set_engine("lm")
```

---

## 4. Define Model Recipe (including our formula)

```{r}
linear_recipe <- 
  recipe(seat.change ~ approval + rdi.change, data = midterms_train)
```


---

## 5. Create a Workflow ...

```{r}
linear_workflow <- 
  workflow() |>
  add_model(linear_spec) |>
  add_recipe(linear_recipe)
```

## 6. ... and fit the model

```{r}
linear_fit <- 
  linear_workflow |>
  fit(data = midterms_train)
```

---

## Check the parameters

```{r}
tidy(linear_fit)
```

---

## 7. Evaluate Model Performance... on train data

```{r}
linear_predictions <- 
  linear_fit |>
  predict(midterms_train) |>
  dplyr::bind_cols(midterms_train)
head(linear_predictions)
```

---

## What is $R^2$ again?

- $R^2$, or the **coefficient of determination**, is a statistical metric used to evaluate the performance of a regression model.
- It indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.

$$R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}}$$

where:
- $\text{SS}_{\text{res}}$: Sum of squared residuals (unexplained variance)
- $\text{SS}_{\text{tot}}$: Total sum of squares (total variance in the outcome)

---

## Interpretation of $R^2$

- **$R^2 = 1$**: Perfect fit. The model explains all variability in the data.
- **$R^2 = 0$**: No fit. The model explains none of the variability.
- **Between 0 and 1**: Partially explains the variance.
    - For example, $R^2 = 0.75$ implies that 75% of the variability in the outcome is explained by the model.

## Limitations
- Adding more variables can increase $R^2$, even if they don’t improve the model significantly. Adjusted $R^2$ can help address this issue.

---

## What is the $R^2$ on our fit?

Use the `rsq_trad` function to find it: where `truth`, or the ground truth value is the observed value and `estimate` is your prediction.

```{r}
rsq_trad(linear_predictions, truth = seat.change, estimate = .pred)
```

---

## What is RMSE?

- **Root Mean Square Error (RMSE)** is a metric that measures the average magnitude of the errors in predictions made by a regression model.

- It represents the square root of the average of squared differences between predicted and actual values.

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

where:
- $n$: Number of observations
- $y_i$: Actual value of the outcome for observation $i$
- $\hat{y}_i$: Predicted value of the outcome for observation $i$

---

## Interpretation

- RMSE is always non-negative, with lower values indicating a better fit.
- An RMSE of 0 means perfect prediction accuracy.
- **Units**: RMSE is in the same units as the dependent variable, making it interpretable in the context of the data.

## Limitations
- RMSE is sensitive to outliers since errors are squared.
- It doesn’t provide an interpretation for how large or small the error is in relative terms; comparing RMSE across different datasets or contexts may require normalization.

---

## What is RMSE on our fit?

```{r}
rmse(linear_predictions, truth = seat.change, estimate = .pred)
```

Use the `rmse` function to find it: where `truth`, or the ground truth value is the observed value and `estimate` is your prediction.

---

## Importance of Evaluation Metrics in Machine Learning

### Why Evaluation Metrics Matter

.small[

- **Assess Model Performance**: Metrics like accuracy, RMSE, and $R^2$ help quantify how well a model predicts outcomes, guiding improvements.

- **Compare Models**: Metrics allow us to objectively compare different models to find the best fit for the problem.

- **Understand Errors**: By analyzing metrics such as precision, recall, or RMSE, we understand where a model may mispredict, enabling targeted refinements.

- **Avoid Overfitting/Underfitting**: Metrics reveal if a model generalizes well to new data, helping avoid models that only perform well on the training set.

- **Select Relevant Metrics**: Choosing the right metric aligns with the specific goals of a task (e.g., accuracy for classification, RMSE for regression), optimizing model impact.

]

---

## 7. Evaluate Model Performance... on test data!

```{r}
linear_predictions <- 
  linear_fit |>
  predict(midterms_test) |>
  dplyr::bind_cols(midterms_test)
```

```{r}
ggplot(linear_predictions) +
  geom_point(aes(x = seat.change, y = .pred)) +
  geom_abline() +
  lims(x = c(-63,63), y = c(-63,63))
```

---
class: inverse, center, middle

# Individual quiz/tutorial Part I (up to "R2 and RMSE")

```{r echo = FALSE}
library(countdown)

countdown(minutes = 10, seconds = 00)
```


---
class: inverse, center, middle

# Text analysis: Regression

---

- We now turn to using text analysis and machine learning to predict some value based on the features we extract from some text.

## Let's go back to the debate transcripts we used last week


---

## 1. Load and prepare data

... Since text analysis is computationally intense we also reduce the size of the data

```{r}
load("../data/debate_transcripts.rda")

## Let's reduce the data set size
debate_transcripts <-
  debate_transcripts |>
  dplyr::sample_n(1000)
```


---

## Know your data

```{r}
debate_transcripts |>
  dplyr::count(election_year) |>
  ggplot(aes(election_year, n)) + 
  geom_col() + labs(x = "year", y = "Number of speeches per year")
```

---

## 2. Splitting the data

```{r}
debate_split <- initial_split(debate_transcripts)

debate_train <- training(debate_split) 
nrow(debate_train)
debate_test <- testing(debate_split)
nrow(debate_test)
```

---

## 4. Define Model Specification

... This is exactly as before

```{r}
linear_spec <- 
  linear_reg() |>
  set_engine("lm") |>
  set_mode("regression")
```

---

## 4. Define Model Recipe (including our formula)

This part is sligthly different because we extract features from text!

- We use the same text processing steps see last week 

- We use the textrecipes package to do it within the tidymodels framework

- Note that you can combine text features and other features in your model (e.g., date of debate)

```{r}
# install.packages('textrecipes')
library(textrecipes)

debate_recipe <- 
  recipe(election_year ~ text, data = debate_train) |> 
  step_tokenize(text) |>
  step_tokenfilter(text, max_tokens = 1000) |>
  step_tf(text) |>
  step_normalize(all_predictors())
```

---

## 5. Create a Workflow ...

```{r}
linear_workflow <- 
  workflow() |>
  add_model(linear_spec) |>
  add_recipe(debate_recipe)
```


## 6. ... and fit the model

```{r}
linear_fit <- 
  linear_workflow |>
  fit(data = debate_train)
```

---

## Let's Explore the results

### What are the tokens that predict more recent years (remember formula `election_year ~ text`)

```{r}
tidy(linear_fit) |>
  dplyr::arrange(-estimate)
```

---

### And What are the tokens that predict further in the past (remember formula `election_year ~ text`)

```{r}
## Further in the past
tidy(linear_fit) |>
  arrange(estimate)
```

---

## 7. Evaluate Model Performance... on train data

```{r}
linear_predictions <- 
  linear_fit |>
  predict(debate_train) |>
  dplyr::bind_cols(debate_train)
```

---

## $R^2$

```{r}
rsq_trad(linear_predictions, truth = election_year, estimate = .pred)
```

## $RMSE$

```{r}
rmse(linear_predictions, truth = election_year, estimate = .pred)
```


---

## Can we do better? 

- Machine learning is all about fine-tuning your workflow to improve the model.

- That's why is important to fine tune it on the train set!

### We could try to remove stopwords and us TF-IDF instead of TF and see if the prediction improves!

---

## With tidymodels, I don't need to redefine everything.

- I only need to intervene on the step that I want to change...

---

## 4. Define Model Recipe (including our formula)

```{r}
debate_recipe <- 
  recipe(election_year ~ text, data = debate_train) |> 
  step_tokenize(text) |>
  step_stopwords() |> # Removed the stop words #<< 
  step_tokenfilter(text, max_tokens = 1000) |>
  step_tfidf(text) |> # Change to tfidf #<< 
  step_normalize(all_predictors())
```

---

## 5. Re-Create a Workflow ...

... with new `debate_recipe`

```{r}
linear_workflow <- 
  workflow() |>
  add_model(linear_spec) |>
  add_recipe(debate_recipe)
```


## 6. ... and fit the model

```{r}
linear_fit <- 
  linear_workflow |>
  fit(data = debate_train)
```

---

## 7. Now we compare the new evaluation metrics with the old

```{r}
linear_predictions <- 
  linear_fit |>
  predict(debate_train) |>
  dplyr::bind_cols(debate_train)
```

```{r}
rsq_trad(linear_predictions, truth = election_year, estimate = .pred)
```

```{r}
rmse(linear_predictions, truth = election_year, estimate = .pred)
```



---
class: inverse, center, middle

# Text analysis: Classification

---

## 1. Load and prepare data

... We have already loaded the data. Let's only focus on classifying two speakers...

```{r}
debate_clinton_trump <-
  debate_transcripts |>
  dplyr::filter(speaker %in% 
                  c("Hillary Clinton", "Donald Trump")) |>
  dplyr::mutate(speaker = factor(speaker))
```

---

## 2. Splitting the data

```{r}
debate_split <- initial_split(debate_clinton_trump, prop = 1/2)  
debate_train <- training(debate_split) 
nrow(debate_train)
debate_test <- testing(debate_split)
nrow(debate_test)
```

---

## 3. Define Model Specification for text

```{r}
debate_recipe <- 
  recipe(speaker ~ text, data = debate_train) %>% 
  step_tokenize(text) %>% 
  step_tokenfilter(text, max_tokens = 1000) %>% 
  step_tfidf(text) %>% 
  step_normalize(all_predictors())
```

## 4. Define Model Recipe (including our formula)

```{r}
logistic_spec <- 
  logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")
```

---

## 5. Create a Workflow ...

```{r}
logistic_workflow <- 
  workflow() |>
  add_model(logistic_spec) |>
  add_recipe(debate_recipe)
```

## 6. ... and fit the model

```{r}
logistic_fit <- 
  logistic_workflow |>
  fit(data = debate_train)
```

---

## Let's explore the results on our test data!

```{r}
logistic_predictions <- 
  logistic_fit |>
  predict(debate_test) |>
  dplyr::bind_cols(debate_test)
logistic_predictions
```

---

## 7. Evaluate the model on test data

- Accuracy

```{r}
accuracy(logistic_predictions, truth = speaker, estimate = .pred_class)
```

- And confusion matrix

```{r}
conf_mat(logistic_predictions, truth = speaker, estimate = .pred_class)
```


---
class: inverse, center, middle

# Individual quiz/tutorial Part II

```{r echo = FALSE}
library(countdown)

countdown(minutes = 10, seconds = 00)
```


---
class: inverse, center, middle

# Attendance

---
class: inverse, center, middle

# See you next week for Network and Spatial Data!


