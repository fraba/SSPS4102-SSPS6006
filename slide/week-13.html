<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>SSPS [4102|6006] Data Analytics[in the Social Sciences|for Social Research]</title>
    <meta charset="utf-8" />
    <meta name="author" content="Francesco Bailo" />
    <script src="week-13_files/header-attrs/header-attrs.js"></script>
    <link href="week-13_files/remark-css/default.css" rel="stylesheet" />
    <link href="week-13_files/countdown/countdown.css" rel="stylesheet" />
    <script src="week-13_files/countdown/countdown.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    <link rel="stylesheet" href="assets/sydney-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/sydney.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# SSPS [4102|6006] Data Analytics</br>[in the Social Sciences|for Social Research]
]
.subtitle[
## Week 13</br>Ethical Considerations and Future Trends
]
.author[
### Francesco Bailo
]
.date[
### Semester 1, 2025 (updated: 2025-05-29)
]

---


background-image: url('assets/USydLogo.svg')
background-size: 95%

&lt;style&gt;
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
&lt;/style&gt;



---
class: inverse, center, middle

# Ethical Considerations and Future Trends

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.

---

## Recap from last week

- Research question: How to write
- Data analysis in R for A3
- Network analysis
- Spatial analysis

---
class: inverse, center, middle

# Any questions about anything?

---

## Plan for today

- Interpreting Regression Coefficients (recap) w/t Interaction Terms (bonus)
- Individual on A3 with peer-review
- The Replication Crisis and P-Values (again)
- The Ethics of Artificial Intelligence
- Data Surfacing
- Mixed-Methods Research
- Workshopping A3


---
class: segue-red

# Constructing research questions, testing hypothesis and interpeting  regressions coefficients (w/t interaction term)

---

## Theory first, always 

### Back off for a sec, quantitative analysis, we are (social) scientists!


.center[&lt;img src = 'https://www.reactiongifs.com/wp-content/uploads/2012/12/bill_murray_ghostbusters.gif'&gt;]

### And we are guided by theory. Always.

---

## So what is the theory?

What we know about `\(Education\)`, `\(Gender\)` and `\(Income\)`? 

.pull-left[

1. We know that going to school, getting more `\(Education\)` years, increase our income because we *know* more and we can *do* more.  

2. We know that women earn less because of multiple contributing factors, including

- **Part-time work**:  Women do more unpaid work, so more work part-time (28% vs. 8% men).



]

.pull-right[

- **Family impact**:  Women take more career breaks for childcare (1 in 3 vs. 1.3% men).

- **Low-paid sectors**:  Women are over-represented in lower-paid fields like care, health, education.

- **Fewer managers**:  Fewer women in top jobs (35% of managers), and female managers earn 23% less.

]


.footnote[Source: https://www.europarl.europa.eu/topics/en/article/20200109STO69925/understanding-the-gender-pay-gap-definition-facts-and-causes]

---

3\. We know "that women experience larger relative returns to higher education, indicating that women have more to gain from investing in education" (Seehuus &amp; Strømme, 2025).

### But how do we model this?

- More `\(Education\)` causes more `\(Income\)`;

- `\(GenderMale\)` causes more `\(Income\)`;

*BUT*

- `\(GenderFemale\)` in interaction with more `\(Education\)` causes more `\(Income\)`.


.footnote[Seehuus, S., &amp; Strømme, T. B. (2025). Gendered Returns to Education: The Association between Educational Attainment, Gender Composition in Field of Study and Income. *Sociology*, 0(0). https://doi.org/10.1177/00380385241303448]


---

## Let's first simulate some data based on this theory...

Note: That we simulate data so to define `\(true\)` coefficients `\(\beta\)` instead of  `\(\widehat{estimated}\)` coefficients `\(\hat{\beta}\)`. 


``` r
set.seed(123) # This to replicate same numbers

# Simulate 1000 observations
n &lt;- 1000
Education &lt;- sample(8:20, n, replace = TRUE)  # between 8 and 20 years
Gender &lt;- factor(sample(c("Male", "Female"), n, replace = TRUE))

# Define true coefficients
intercept &lt;- 20000 # Baseline, Female with 0 years of education.
b_Education &lt;- 1000 # Proposition 1
b_GenderFemale &lt;- -5000  # Proposition 2
b_Interaction &lt;- 100 # Proposition 3
```


---

## Let's add an true interaction effect to the simulated data

(More on this later)


``` r
# Simulate Income with interaction
Income &lt;- intercept +
  b_Education * Education + # Proposition 1
  ifelse(Gender == "Female", b_GenderFemale, 0) + # Proposition 2
  ifelse(Gender == "Female", b_Interaction * Education, 0) + # Proposition 3
  rnorm(n, mean = 0, sd = 5000)  # add noise

# Create data frame
data &lt;- data.frame(Income, Education, Gender)
head(data)
```

```
##     Income Education Gender
## 1 21072.30        10 Female
## 2 30419.65        10   Male
## 3 27100.02        17 Female
## 4 29806.13         9   Male
## 5 29875.36        13   Male
## 6 39585.82        18 Female
```


---

### Summary stats (always summarise your variables and their critical relationships)

`\(Education\)` summary stats, not that interesting since we simulate `\(Education\)` to be independent from `\(Gender\)`. 


``` r
data |&gt;
  dplyr::group_by(Gender) |&gt;
  dplyr::summarise(mean(Education), sd(Education))
```

```
## # A tibble: 2 × 3
##   Gender `mean(Education)` `sd(Education)`
##   &lt;fct&gt;              &lt;dbl&gt;           &lt;dbl&gt;
## 1 Female              14.1            3.70
## 2 Male                14.1            3.58
```

---


``` r
ggplot(data) +
  geom_density(aes(x = Education)) +
  facet_grid(Gender~.) +
  geom_vline(data = data %&gt;% group_by(Gender) %&gt;% 
               summarise(mean=mean(Education)), 
             aes(xintercept=mean))
```

&lt;img src="week-13_files/figure-html/unnamed-chunk-4-1.svg" width="50%" style="display: block; margin: auto;" /&gt;


---

`\(Income\)` ~ `\(Gender\)` is instead more interesting...


``` r
data |&gt;
  dplyr::group_by(Gender) |&gt;
  dplyr::summarise(mean(Income), sd(Income))
```

```
## # A tibble: 2 × 3
##   Gender `mean(Income)` `sd(Income)`
##   &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;
## 1 Female         30416.        6289.
## 2 Male           34418.        6154.
```

---

``` r
ggplot(data) +
  geom_density(aes(x = Income)) +
  facet_grid(Gender~.) +
  geom_vline(data = data %&gt;% group_by(Gender) %&gt;% 
               summarise(mean=mean(Income)), 
             aes(xintercept=mean))
```

&lt;img src="week-13_files/figure-html/unnamed-chunk-6-1.svg" width="50%" style="display: block; margin: auto;" /&gt;

---

.pull-left[

&lt;img src="week-13_files/figure-html/unnamed-chunk-7-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

]

.pull-right[

### We note two things

1. Expected (mean) `\(Income\)` is higher for `\(GenderMale\)` than for `\(GenderFemale\)`. 

2. The shape of the two curves is different. This is because something is going on in the relationship between `\(GenderFemale\)` and `\(Income\)` that is not simply explained by a linear relationship between `\(Income\)` and `\(Education\)`. 


]

---
class: segue-red

# Let's analyse our data with regression analysis!

---

## What Is a Linear Regression?

- **Linear regression** models the relationship between a dependent variable `\(y\)` and one or more independent variables `\(x_1, x_2, \dots, x_p\)`.
  
- The general form:
  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
  $$
  where:
  - `\(\beta_0\)` = intercept (value of `\(y\)` when all `\(x\)` = 0)
  - `\(\beta_i\)` = coefficient (effect of `\(x_i\)` on `\(y\)`)
  - `\(\epsilon\)` = error term (unexplained variation)

- **Goal**: Find the best-fitting (straight) line that minimises the sum of squared errors between predicted and actual `\(y\)`.

- Example in R:
  ```r
  model &lt;- lm(y ~ x1 + x2, data = mydata)
  ```

---

### What Is a Regression Coefficient?

- A **coefficient** tells us:
  → How much `\(y\)` changes when predictor `\(x\)` increases by 1, **holding others constant**.
- Example in R:
  ```r
  model &lt;- lm(y ~ x1, data = mydata)
  summary(model)
  ```
- If `\(\beta_1 = 2\)`, then for each +1 unit in `\(x_1\)`, `\(y\)` increases by 2.

&lt;img src="week-13_files/figure-html/unnamed-chunk-8-1.svg" width="35%" style="display: block; margin: auto;" /&gt;

---

## Interpreting the Intercept

- The **intercept** `\((\text{Intercept})\)` is the expected value of `\(y\)` when all predictors = 0.
- Example:
  ```r
  model &lt;- lm(Income ~ Education, data = data)
  ```
- Intercept = predicted income when `\(Education = 0\)`.

---

&lt;img src="week-13_files/figure-html/unnamed-chunk-9-1.svg" width="80%" style="display: block; margin: auto;" /&gt;


---

## Adding a Second Predictor

- With two predictors:
  ```r
  model &lt;- lm(y ~ x1 + x2, data = mydata)
  ```
- `\(\beta_1\)`: effect of `\(x_1\)` when `\(x_2\)` is held constant.
- `\(\beta_2\)`: effect of `\(x_2\)` when `\(x_1\)` is held constant.

---

## Introducing Interaction Terms

- Interaction = when the effect of one variable **depends on** another.
- R syntax:
  ```r
  model &lt;- lm(y ~ x1 * x2, data = mydata)
  ```
- Expands to:
  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \times x_2)
  $$
  
---

## Interpreting Interaction Coefficients


  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \times x_2)
  $$


- `\(\beta_1\)`: effect of `\(x_1\)` when `\(x_2 = 0\)`.
- `\(\beta_2\)`: effect of `\(x_2\)` when `\(x_1 = 0\)`.
- `\(\beta_3\)`: **change** in the effect of `\(x_1\)` per +1 unit in `\(x_2\)`.


## But let's go back to our simulated data...

---

## This is what we did to define the true coefficients (a.k.a. paramaters)


``` r
set.seed(123) # This to replicate same numbers

# Simulate 1000 observations
n &lt;- 1000
Education &lt;- sample(8:20, n, replace = TRUE)  # between 8 and 20 years
Gender &lt;- factor(sample(c("Male", "Female"), n, replace = TRUE))

# Define true coefficients
intercept &lt;- 20000 # Baseline, Female with 0 years of education.
b_Education &lt;- 1000 # Proposition 1
b_GenderFemale &lt;- -5000  # Proposition 2
b_Interaction &lt;- 100 # Proposition 3

# Simulate Income with interaction
Income &lt;- intercept +
  b_Education * Education + # Proposition 1
  ifelse(Gender == "Female", b_GenderFemale, 0) + # Proposition 2
  ifelse(Gender == "Female", b_Interaction * Education, 0) + # Proposition 3
  rnorm(n, mean = 0, sd = 5000)  # add noise

data &lt;- data.frame(Income, Education, Gender)
```


---

## Run the Regression and Get Estimated Coefficients

Note: 

- *True* coefficients `\(\neq\)` `\(\widehat{estimated}\)` coefficients. 

- *True* coef. `\(= \widehat{estimated}\)` coef. + `\(errors\)` (with `\(errors\)` being uknown).


``` r
model &lt;- lm(Income ~ Education, data = data)
```

---


``` r
model &lt;- lm(Income ~ Education, data = data)
summary(model)
```

```
## 
## Call:
## lm(formula = Income ~ Education, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -16756.0  -3655.7    119.8   3653.9  18510.6 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18013.91     679.57   26.51   &lt;2e-16 ***
## Education    1025.14      46.61   22.00   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5362 on 998 degrees of freedom
## Multiple R-squared:  0.3265,	Adjusted R-squared:  0.3258 
## F-statistic: 483.8 on 1 and 998 DF,  p-value: &lt; 2.2e-16
```

Note: Because we simulated the data we knew that `\(\beta=1000\)` dollars. Now we have the estimated coefficients: `\(\hat{\beta} = 1025.138834\)` dollars. 

---

## Let's add `\(Gender\)` as second predictor! 


``` r
model &lt;- lm(Income ~ Education + Gender, data = data)
summary(model)
```

```
## 
## Call:
## lm(formula = Income ~ Education + Gender, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14680.6  -3462.7    -20.1   3540.5  16561.2 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 15894.92     651.34   24.40   &lt;2e-16 ***
## Education    1027.32      43.21   23.78   &lt;2e-16 ***
## GenderMale   4031.28     314.58   12.81   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4971 on 997 degrees of freedom
## Multiple R-squared:  0.4217,	Adjusted R-squared:  0.4206 
## F-statistic: 363.6 on 2 and 997 DF,  p-value: &lt; 2.2e-16
```

---

- Our true coefficient `\(\beta\)` for `\(GenderFemale\)` was -5000 dollars, therefore for  `\(GenderMale\)` is +5000.

- Based on our model, the estimated coefficient `\(\hat{\beta}\)` for `\(GenderMale\)` is 4031.2765746.


``` r
model
```

```
## 
## Call:
## lm(formula = Income ~ Education + Gender, data = data)
## 
## Coefficients:
## (Intercept)    Education   GenderMale  
##       15895         1027         4031
```

---

## Let's add an interaction term between `\(Gender\)` and `\(Education\)`

Yet, our theory suggests that an interaction between `\(Gender\)` and `\(Education\)` in affecting `\(Income\)`: 

- Women benefit more than men from *additional* education in terms of income.


``` r
model &lt;- lm(Income ~ Gender * Education, data = data)
```

- Instead of `+` I know use `*`.

---


``` r
data &lt;- 
  data |&gt;
  dplyr::mutate(Gender = factor(Gender, 
                                levels = c("Male", "Female")))
```



``` r
model &lt;- lm(Income ~ Gender * Education, data = data)
summary(model)
```

```
## 
## Call:
## lm(formula = Income ~ Gender * Education, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14794.7  -3439.1    -49.2   3520.1  16617.3 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            20199.28     888.56  22.733  &lt; 2e-16 ***
## GenderFemale           -4579.29    1260.61  -3.633 0.000295 ***
## Education               1007.96      61.06  16.509  &lt; 2e-16 ***
## GenderFemale:Education    38.81      86.45   0.449 0.653578    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4973 on 996 degrees of freedom
## Multiple R-squared:  0.4219,	Adjusted R-squared:  0.4201 
## F-statistic: 242.3 on 3 and 996 DF,  p-value: &lt; 2.2e-16
```

---

## Interpreting the coefficients in the presence of an interaction term



|                       |   est|  true|description                                          |
|:----------------------|-----:|-----:|:----------------------------------------------------|
|(Intercept)            | 20199| 20000|Male baseline with 0 years in education              |
|GenderFemale           | -4579| -5000|What Females earns with 0 years of education         |
|Education              |  1008|  1000|What Male earns per year of education                |
|GenderFemale:Education |    39|   100|Extra Income per extra year of education for Females |

---

So, when yo have an interaction term:

`$$\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{1}x_{2}$$`
.small[

* `\(\beta_1\)`: the marginal effect of `\(X_{i1}\)` on predicted `\(Y_i\)` when `\(X_{i2} = 0\)`.
* `\(\beta_2\)`: the marginal effect of `\(X_{i2}\)` on predicted `\(Y_i\)` when `\(X_{i1} = 0\)`.
* `\(\beta_3\)`: the change in the marginal effect of `\(X_{i1}\)` due to a one-unit change in `\(X_{i2}\)` **OR** the change in the marginal effect of `\(X_{i2}\)` due to a one-unit change in `\(X_{i1}\)`.

&gt; Interactions are a standard part of social science research because they allow us to assess how the relationship between the outcome and an independent variable varies by the values of another variable. In the context of our study of wait times at a voting precinct, if `\(X_{i1}\)` is income and `\(X_{i2}\)` is the Black/non-Black voter indicator, then `\(\beta_3\)` represents the change in the slope of the wait time-income relationship between Black and non-Black voters.  

]

.footnote[This slide is from "A User’s Guide to Statistical Inference and Regression" by Matthew Blackwell. Source: https://mattblackwell.github.io/gov2002-book/]

---

## Centering variables to improve interpretability

.small[

In many cases, the so-called marginal coefficients on the lower-order terms ( `\(\beta_1\)` for `\(X_{i1}\)` and `\(\beta_2\)` for `\(X_{i2}\)` ) are uninteresting because they represent the marginal effect of one variable when the other is 0. If `\(X_{i1}\)` is age and `\(X_{i2}\)` is the Black/non-Black indicator, then `\(\beta_2\)` is the estimated difference in average voter wait times for voters who are zero years old, an obviously nonsensical parameter. We can improve the interpretability of the coefficient by recentering the age variable. Suppose we include a mean-centered version of age, 

`$$\widetilde{X}_{i1} = X_{i1} - \overline{X}_1$$`

in place of `\(X_{i1}\)`. That is, we regress `\(Y_i\)` on `\(\widetilde{X}_{i1}\)`, `\(X_{i2}\)`, and `\(\widetilde{X}_{i1}X_{i2}\)`. In this case, `\(\beta_2\)` (the coefficient on the race indicator `\(X_{i2}\)` ) is the marginal effect of `\(X_{i2}\)` when `\(\widetilde{X}_{i1} = 0\)` or when `\(X_{i1} = \overline{X}_1\)`. Thus, this coefficient is now the estimated difference in average voter wait times for the average-aged voter, which is far more interpretable. This recentering has no effect on either `\(\beta_1\)` or `\(\beta_3\)`, which is rather remarkable.

]

.footnote[This slide is from "A User’s Guide to Statistical Inference and Regression" by Matthew Blackwell. Source: https://mattblackwell.github.io/gov2002-book/]

---

class: inverse, center, middle

# Individual on A3 with peer-review (from Canvas)

<div class="countdown" id="timer_20491153" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">20</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
class: inverse, center, middle

# The Replication Crisis and P-Values

---

## The Replication Crisis

- Began around 2011, affecting fields like psychology and social sciences.

- Defined by the inability to replicate results in studies, which questions the reliability of scientific findings.

- High-profile cases like Diederik Stapel, a social psychologist, **fabricated data** in studies on behavior, highlighting broader issues.

- But the **replication crisis** is not only about fraud... 

.footnote[Slides adapted from Chivers, T. (2024). *Everything is predictable: How Bayesian statistics explain our world*. One Signal Publishers/Atria.]

---

## P-Values: Misunderstood Statistic

- **What is a p-value?**  
  - Represents the probability of obtaining test results at least as extreme as the observed data, assuming the null hypothesis is true.
- **Misinterpretation:**  
  - Many scientists believe a p-value (e.g., p &lt; 0.05) indicates the probability that the hypothesis is true, which is incorrect.  
- Studies show a significant percentage of scientists misunderstand p-values, which impacts the credibility of research.

---

## Let's replicate a famous study on the misconceptions in intepreting the significance test (i.e., the p-value)


On Canvas take, the 

.center[.content-box-yellow[*Week 13 Individual quiz (In-class)*]]

(it's anonymous).

<div class="countdown" id="timer_87234ae7" data-update-every="1" tabindex="0" style="right:0;bottom:0;">
<div class="countdown-controls"><button class="countdown-bump-down">&minus;</button><button class="countdown-bump-up">&plus;</button></div>
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

The very same questions you just answered were got mostly wrong in two famous studies:

Percentages of participants in each group who made at least one mistake, in Haller &amp; Krauss, (2002) and Oakes (1986)

&lt;img src="week-13_files/figure-html/unnamed-chunk-21-1.svg" width="70%" style="display: block; margin: auto;" /&gt;


.footnote[
- Haller, H., &amp; Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. *Methods of Psychological Research*, 7(1), 1–20.
- Oakes, M. (1986). *Statistical inference: A commentary for the social and behavioral  sciences*, Chichester: Wiley.]



---

## Again ...

A p-value of, let's say, 0.05 means that 

&gt; The probability of the available (or of even less likely) data, given that the null hypothesis is true, is less than 5%.

.center[.content-box-yellow[P-Values just help you understand how rare the results are.]]

---

## Let's check why the first six statements were wrong...

- 1# You have absolutely disproved the null hypothesis (that is, there is no difference between being white or non-white in the hiring process).

- 3# You have absolutely proved your experimental hypothesis (that there is a difference between being white or non-white).

&gt; Statements 1 and 3 are easily classified as being false: **Significance tests can never prove (or disprove) hypotheses**. Significance tests provide probabilistic information and  can, therefore, at best be used to **corroborate theories**.

.footnote[Haller, H., &amp; Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. *Methods of Psychological Research*, 7(1), 1–20.]

---

- 2# You have found the probability of the null hypothesis being true.

- 4# You can deduce the probability of the experimental hypothesis being true.

- 5# You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.

&gt; It is generally impossible to assign a probability to any hypothesis by applying significance tests: One can neither assign it a probability of 1 (statements 1 and 3) nor any  other probability (statements 2, 4 and 5).

.footnote[Haller, H., &amp; Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. *Methods of Psychological Research*, 7(1), 1–20.]

---

*This is the trickiest...*

- 6# You have a reliable experimental finding in the sense that if, hypothetically, the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.

&gt; Statement 6 reflects the so-called "replication fallacy" ... one could interpret `\(\alpha\)` = .01 in a frequentistic framework **as relative frequency of rejections** of `\(H_0\)` if `\(H_0\)` **is true**. The **example however gives no evidence of the `\(H_0\)` being true**. "In the minds of many, 1 - p erroneously turned into the relative frequency of rejections of `\(H_0\)`, that is, into the probability that significant results could be replicated"  (Gigerenzer, 1993a).

.footnote[Haller, H., &amp; Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. *Methods of Psychological Research*, 7(1), 1–20.]


---

## What is the replication fallacy?

- **Replication fallacy**:  
  The mistaken belief that:
  
  &gt; If you get \( p = 0.05 \), you have a 95% chance  of replicating the result.

- People assume:
  
  $$
  \text{Replication probability} = 1 - p
  $$

- This is **wrong** — let’s break down why.

---

## Step 1: What does a p-value actually mean?

- `\(p = 0.05\)` means:
  
  &gt; Assuming the **null hypothesis** `\(H_0\)` is true,  
  there’s a 5% chance of seeing this result  
  (or something more extreme).

- It does **not** tell us:
  - The probability that `\(H_0\)` is true.
  - The probability the result is “real.”
  - The probability the result will replicate.

---

## Step 2: Why can’t we just use `\(1 - p\)`?

- Even if the original finding is significant,  replication depends on:
  
  - The **true effect size**.  
  - The **statistical power** of the test.  
  - The **sampling variability**.

- A small p-value doesn’t guarantee that the effect is strong or reproducible.

---

## Step 3: An intuitive example

- Imagine a study with **50% power**.

- Even if the effect is real, you only have a **50% chance** to detect it again.

- The p-value only tells you how surprising the original result was under `\(H_0\)`, not how robust it is.

---

## Step 4: Why do people fall for the fallacy?

- Common misunderstandings:
  - `\(p = 0.05\)` → 95% chance the effect is real.
  - `\(p = 0.01\)` → 99% chance it will replicate.

- Reality:
  - Replication = depends on power, effect size, noise.
  - Many significant findings have low power and are unlikely to replicate.

---

## Step 5: Coin flip example

Null hypothesis: This coin is fair. Alternative hypothesis: This coin is biased. 

- Flip coin 10 times.

- What’s the probability of getting ≥8 heads?

  $$
  P(X \geq 8) \approx 0.0547
  $$

- Two-tailed test (include ≤2 tails):

  $$
  2 \times 0.0547 \approx 0.109
  $$

- Not significant at `\(p = 0.05\)` —  you’d need ≥9 heads for `\(p \approx 0.02\)`.

---

### The coin flip example shows:

- Even with relatively rare outcomes (like 8/10 heads), you might **not** reach statistical significance.

- To cross the `\(p &lt; 0.05\)` threshold, you need even more extreme outcomes (like 9/10 heads).

---

# How it ties to replication

Let’s say you flip 10 coins and this time you get 9 heads:

- You calculate `\(p \approx 0.02\)` → significant.

If you believe in the replication fallacy, you think:

&gt; “There’s a 98% chance I’ll get another significant result if I repeat this.”

But that’s wrong — because:

- Your chance of getting another 9+ heads is in fact only ~1% under the null.
- Even under the alternative (if the coin is slightly biased), the **power** of the test might still be low, meaning your replication success is much less than 98%.

---

# Small sample + extreme result = fragile finding

In small-sample experiments (like 10 trials):

- Significant results often hinge on rare, extreme patterns.
- The chance of seeing that same extreme again is small, even if the underlying effect is real.

This is exactly why **p-values do NOT directly tell you replication probability**.

---

## Step 6: Key takeaway

- **P-values do not equal replication probabilities.**

- Replication success depends on:
  - Statistical power.
  - True effect size.
  - Experimental design and variability.

- This is why the replication crisis exists — we overestimate how robust single findings are.


---

## Problems with P-Values in Research
- **p &lt; 0.05 Threshold**: Often used as a benchmark for "statistical significance" in research.
  - A result of p &lt; 0.05 suggests the observed data would occur by chance only 5% of the time if the null hypothesis were true.
- **Misleading Significance**:  
  - A low p-value **does not confirm the hypothesis**; it merely indicates that the data is unlikely under the null hypothesis.

---

## Case Study: False Positive Psychology
- **Joseph Simmons et al. (2011)**: Used p-values to show "impossible" results.
  - Experiment found that listening to The Beatles’ *When I’m Sixty-Four* made participants "younger" (p = 0.04).
  - Revealed flaws in the p-value system, showing that traditional statistical methods can yield false positives.
  
  
.footnote[Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. *Psychological Science*, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632
]  

---

.small[

| Researcher degrees of freedom                        | Significance level       |           |           |
|-----------------------------------------------------|---------------------------|-----------|-----------|
|                                                     | p &lt; .1                    | p &lt; .05   | p &lt; .01   |
| **Situation A**: two dependent variables (r = .50)  | 17.8%                     | 9.5%      | 2.2%      |
| **Situation B**: addition of 10 more observations per cell | 14.5%              | 7.7%      | 1.6%      |
| **Situation C**: controlling for gender or interaction of gender with treatment | 21.6% | 11.7%     | 2.7%      |
| **Situation D**: dropping (or not dropping) one of three conditions | 23.2%       | 12.6%     | 2.8%      |
| **Combine Situations A and B**                      | 26.0%                     | 14.4%     | 3.3%      |
| **Combine Situations A, B, and C**                  | 50.9%                     | 30.9%     | 8.4%      |
| **Combine Situations A, B, C, and D**               | 81.5%                     | 60.7%     | 21.5%     |

]

.footnote[Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. *Psychological Science*, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632
]  

---

## P-Hacking: Manipulating Data for p &lt; 0.05
- **P-Hacking Defined**:  
  - Adjusting data, experimenting with various statistical methods, or selectively reporting results to achieve p &lt; 0.05.
- **Common Techniques**:  
  - Stopping data collection once significance is reached.
  - Selectively including/excluding variables.
  - Repeating tests until significance is found.
- These practices lead to biased and potentially false conclusions in research.

---

## P-Hacking: Real-World Example
- **Brian Wansink’s Food Psychology Studies**  
  - Wansink's team performed analyses in multiple ways to “mine” the data for p &lt; 0.05 results.
  - Example: “Men eat more in the presence of women” — a result reached by manipulating dataset splits.
- Result: Over a dozen of Wansink’s papers retracted due to unreliable methods.

---

## HARKing: Hypothesizing After Results Are Known
- **HARKing Defined**:  
  - Formulating hypotheses based on the data collected, instead of predefining them before analysis.
- **Impact on Science**:  
  - Encourages “fitting” the data to hypotheses, making it easier to publish “significant” findings.
  - Contributes to the replication crisis by distorting study findings.

---

## The Reproducibility Project (2015)
- **Brian Nosek’s Reproducibility Project**  
  - Attempted to replicate 100 psychology studies.
  - Results: Only 36 out of 100 studies achieved significant results upon replication.
- **Conclusion**:  
  - Suggests that many published findings might be false positives, influenced by p-hacking and reliance on p &lt; 0.05 as a significance threshold.

---

## Bayesian Approach: A Solution to p-Value Limitations
- **Bayesian Analysis**: Uses prior knowledge or "priors" and updates beliefs with new data.
  - Bayesian methods integrate previous knowledge, reducing the need to find p &lt; 0.05 results in each new study (as you do with a *frequentist* approach).
- **Advantages Over P-Values**:
  - Allows for accumulating evidence over time.
  - Avoids arbitrary thresholds like p &lt; 0.05, reducing false positives and p-hacking incentives.

---

## Prior Probabilities and Plausibility
- Bayesian methods consider the *prior probability* of a hypothesis being true.
  - For implausible hypotheses, more data is required to "overcome" prior skepticism.
- **Contrast with p-Values**:  
  - P-values treat all hypotheses equally, ignoring whether they’re plausible, potentially leading to the publication of "outlandish" claims.
  
---

## Optional Stopping: How Bayesian Methods Address It
- **Frequentist Issue**: Checking results multiple times increases false-positive risk.
  - “Optional stopping” refers to stopping data collection once p &lt; 0.05 is achieved.
- **Bayesian Approach**: Can incorporate new data without inflating false positives.
  - Results update gradually, reducing impact of early “significant” results and helping maintain study integrity.

---

## Reducing the p-Value Threshold: Proposed Solution
- **Suggestion**: Lower p-value threshold from 0.05 to 0.005.
  - Would reduce false positives by making it harder to reach “significance.”
- **Challenges**:  
  - While it reduces false positives, it doesn’t address all limitations of p-values, such as not considering prior plausibility.

---

## Summary: Beyond p-Values with Bayesian Methods
- **Limitations of p-values**: Encourage p-hacking, HARKing, and misunderstandings about evidence.
- **Bayesian Solution**:  
  - Provides a more flexible, evidence-based approach.
  - Considers prior knowledge, discourages data manipulation, and fosters reproducible science.

---
class: inverse, center, middle

# Future trend #1

# The Ethics of Artificial Intelligence for Intelligence Analysis: a Review of the Key Challenges with Recommendations

---

## Introduction to Ethical AI in Intelligence Analysis

- **Examples**: The UK's GCHQ and the US CIA are actively developing AI capabilities, aiming to streamline intelligence processes.
- **Purpose**: Highlight ethical issues in using AI for intelligence and provide recommendations for responsible implementation.

### Ethical Challanges of Big Data analyses:

1. Intrusion
2. Explainability and Accountability
3. Bias

.footnote[Blanchard, A., &amp; Taddeo, M. (2023). The ethics of artificial intelligence for intelligence analysis: A review of the key challenges with recommendations. *Digital Society*, 2(1), 12. https://doi.org/10.1007/s44206-023-00036-4]

---

## Ethical Challenge: Intrusion
- **Privacy Concerns**: AI's capacity to process vast datasets raises significant privacy issues, especially regarding the boundaries of surveillance.
- **Bulk Data Collection**: AI enables the collection and analysis of extensive data, sparking debate on when intrusion actually begins.
- **Recommendation**: Focus on purpose-oriented data collection, gathering only data relevant to specific intelligence goals to limit unnecessary intrusion.

---

## Ethical Challenge: Explainability and Accountability
- **Explainability**: Advanced AI (machine learning) systems, like neural networks, often act as "black boxes," making decision-making processes opaque. We saw that machine learning can act on billions of parameters (i.e. variables, how do you *interpret effects* there?)
- **Accountability**: Analysts need transparent AI to confidently defend AI-assisted analysis, especially for decisions with serious implications.
- **Recommendation**: Use interpretable AI models and establish ethics-based auditing practices to ensure accountability and foster trust in AI outputs.

---

## Ethical Challenge: Bias
- **Sources of Bias**: Bias in AI can arise from training data, model design, or operational practices, potentially leading to unfair or discriminatory outcomes.
- **Social Impact**: Bias in intelligence analysis risks reinforcing stereotypes or targeting specific groups, impacting societal justice.
- **Recommendation**: Equip analysts with tools to assess training data for bias, and ensure ongoing evaluations to detect and mitigate algorithmic bias.

--

## Can we think of any examples?

---

## Authoritarianism and Political Security

- **Ethical Challenge**: The use of AI in intelligence analysis can strengthen authoritarian control, raising ethical concerns.
- **Potential for Misuse**: AI’s ability to monitor and control populations could be misused by states to enforce authoritarian policies, limiting political freedoms and civil liberties.
- **Implications for Security**: While AI can enhance security, it risks undermining democratic values if leveraged to suppress dissent or surveil citizens excessively.
- **Recommendation**: Establish safeguards and ethical guidelines to prevent AI from being used to infringe on civil liberties and ensure it aligns with democratic principles.

--

### Can we think of a possible practical scenario?

---

## Conclusion: Responsible Use of AI in Intelligence
- AI offers significant potential to enhance intelligence analysis but poses ethical risks that impact democratic values.
- **Long-Term Strategy**: Implement 

  1. ethical principles, 
  2. prioritise transparency, and 
  3. conduct regular assessments
  
to responsibly integrate AI.
- **Balancing Act**: Ensure national security benefits do not compromise ethical standards, maintaining public trust and democratic integrity.

---
class: inverse, center, middle

# Future trend #2

# Is data surfacing the future of empirical social research?

---

## The Evolution of Empirical Social Research
- **Historical Perspective**: Early social research aimed at systematic data gathering for state use, focusing on health, wealth, and societal status.
- **Shift in Data Use**: Originally, data was collected for tax, trade, or war, but by the 19th century, the focus expanded to predict societal trends.
- **Influence**: The English Utilitarians and French Positivists promoted viewing data as a reflection of social health or pathology.

---

## The Role of Data in a Post-Truth Society
- **Data and Truth**: In today’s "post-truth" world, data producers are numerous, leading to uncertainty over what constitutes truth.
- **Public Resistance**: As people understand research methods, they may withhold or manipulate information, challenging data integrity.
- **Historical Analogy**: Elisabeth Noelle-Neumann’s "spiral of silence" concept describes how dominant voices suppress opposing views, impacting data accuracy.

---

## Big Data and Passive Information Gathering
- **Unobtrusive Measures**: Unlike traditional methods that required active participation, big data passively collects user behavior (e.g., clicks, likes).
- **Metadata Relevance**: Analysts focus on relationships in data (metadata) rather than explicit details, using patterns to infer deeper insights.
- **Challenges**: Empirical research struggles to adapt to this scale and nature of data, which was initially gathered for unintended purposes.

---

## Data Surfacing: A New Approach to Empirical Research

.small[
- **Definition and Purpose**: Data surfacing is a method developed by the analytics firm **Palantir**, designed as an alternative to traditional data mining. Unlike data mining, which extracts specific, predefined insights and treats other data as "noise," data surfacing presumes that *all* data is potentially valuable. This approach visualises comprehensive datasets to allow users to interpret and discover emerging trends and patterns without preconceived constraints.

- **How It Works**: Data surfacing involves displaying all relevant data in a way that encourages users to identify "emergent phenomena" or unexpected insights that may not fit into standard analytical frameworks. This is achieved by relaxing initial assumptions and focusing more on anomalies or directional patterns within the dataset.

- **Comparison to Grounded Theory**: Data surfacing is similar to *grounded theory* in qualitative research. In grounded theory, researchers avoid predefined hypotheses, instead observing subjects' experiences to let themes emerge naturally. Similarly, data surfacing treats large datasets as subjects for open interpretation, rather than filtering data based on existing models or hypotheses.
]

---

## Applications and Benefits 

Data surfacing has applications in fields like national security, healthcare, and market research, where understanding emerging patterns can be crucial. By focusing on the dataset as a whole, this method allows insights that might otherwise be missed, helping researchers and analysts identify novel directions for inquiry.

- **Implications for Researchers**: This approach shifts how researchers interact with data, encouraging them to view it holistically rather than selectively. Data surfacing can uncover "undiscovered public knowledge," as termed by library scientist Don Swanson, by tapping into overlooked or ignored information within existing data.

In summary, data surfacing redefines data analysis by allowing full exploration of datasets without narrowing focus to predefined objectives, fostering innovative insights across disciplines.

---

## Ethical considerations on Data Surfacing (and Data Hoarding)

### What is &lt;img src = 'https://upload.wikimedia.org/wikipedia/commons/1/13/Palantir_Technologies_logo.svg' width = '20%'&gt;

.small[
- **Data Integration and Analysis**: Palantir Technologies specialises in synthesising vast, complex datasets for analysis, helping organisations like governments, intelligence agencies, and corporations make data-driven decisions.
- **Core Software**: 
  - *Gotham*: Primarily used for national security and defence; integrates data for intelligence agencies to identify patterns and prevent threats.
  - *Foundry*: Commercial application used in various sectors, from logistics to healthcare, allowing organisations to visualise and interpret data insights.
- **Use Cases**: 
  - Used by governments for counterterrorism and pandemic response, tracking logistics for the U.S. Army, financial monitoring by Credit Suisse, and production efficiency improvements for Airbus.
]

.footnote[Steinberger, M. (2020, October 21). Does Palantir See Too Much? *The New York Times*. https://www.nytimes.com/interactive/2020/10/21/magazine/palantir-alex-karp.html]

---

## Surveillance Capitalism

.small[

- **Definition**: Surveillance capitalism is a form of information capitalism where human experience is transformed into raw material for data. This behavioural data is processed by machine intelligence to create *prediction products*, which forecast user actions and are sold in *behavioural futures markets*.
  
- **Methods of Data Accumulation**:
  - **Data Exhaust**: Surveillance capitalists collect data exhaust, or residual data from online activities, to improve predictive algorithms and influence user behaviour for profit.
  - **Expanding Data Sources**: They gather diverse data, from physical location to biometric data, covering even intimate aspects like emotions and health metrics.
  - **Network of Coercion**: Users often must share data to access essential functions on devices, such as smart home systems, where opting out can reduce functionality.
  - **Obfuscation Tactics**: Data collection is often presented as essential for “free” services, while technical jargon obscures the full extent of data capture, making it challenging for users to understand the process.

]

---

## The Problem of the Two Texts

.small[

- **Two Types of Texts**:
  - **First Text**: The visible, public-facing information that users create and interact with online, such as social media posts, search history, and shared content.
  - **Shadow Text**: A hidden layer comprising the behavioural data extracted from the first text. This data, accessible only to surveillance capitalists, forms prediction products for commercial use.

- **Implications of the Shadow Text**:
  - **Asymmetrical Knowledge**: Surveillance capitalists gain extensive insight into users, creating a knowledge gap where they understand users better than users understand themselves.
  - **Influencing the First Text**: This hidden information enables companies to manipulate the visible content, such as ordering search results or recommending products, to align with their interests, further deepening the impact of surveillance capitalism.
  
  ]

.footnote[Zuboff, S. (2018). *The age of surveillance capitalism: The fight for a human future at the new frontier of power*. PublicAffairs.]

---
class: inverse, center, middle

# Future trend #3

# Mixed methods research: what it is and what it could be

---

## Introduction to Mixed Methods Research (MMR)

**Overview**: Mixed Methods Research (MMR) is an approach that combines both quantitative and qualitative research methods within a single study or series of studies. This combination allows researchers to capture more comprehensive insights and address complex research questions by leveraging the strengths of both approaches.

**Purpose**: MMR is particularly valuable in social sciences and applied fields, where understanding complex human behaviours and contexts requires both .content-box-yellow[*statistical analysis*] and .content-box-yellow[*contextual understanding*].

.footnote[Timans, R., Wouters, P., &amp; Heilbron, J. (2019). Mixed methods research: What it is and what it could be. *Theory and Society*, 48(2), 193–216. https://doi.org/10.1007/s11186-019-09345-5]

---

## What is Mixed Methods Research?

**Definition**: MMR is a research approach that systematically integrates quantitative and qualitative methods to provide a more complete understanding of research problems than either method could offer independently.

**Key Goal**: To synthesise numerical data with narrative insights, facilitating a more holistic interpretation of data and findings.

**Types of Data**: 
- Quantitative data includes **structured**, numerical data that often seeks to measure or predict variables.
- Qualitative data involves **unstructured** or **semi-structured data**, such as interview transcripts or observational notes, that provides context and depth.

---

## Why Mixed Methods? The Purpose of Combining Approaches

**Purpose**  
- To achieve broader perspectives by combining qualitative depth with quantitative generalisability.
- Each method compensates for the other's limitations, providing robust insights.

**Core Advantages**  
1. **Corroboration** - Confirms findings through triangulation.
2. **Expansion** - Extends understanding by adding layers of interpretation.
3. **Complementarity** - Merges strengths of both approaches.

---

## Key Components of Mixed Methods Research

1. **Data Collection**: Using both qualitative (e.g., interviews) and quantitative (e.g., surveys) methods.
2. **Analysis**: Integrating results to provide a unified interpretation.
3. **Timing**: Methods can be concurrent or sequential.
4. **Emphasis**: Either approach can be dominant, or both can have equal weight.

---

## Institutionalisation and Popularity of MMR

**Growth of MMR**  
- The number of MMR publications and dedicated journals (e.g., Journal of Mixed Methods Research) has surged.
- MMR has achieved substantial institutionalisation, with significant uptake in educational and nursing research.

**Key Milestones**  
- Founding of journals and conferences.
- Establishment of Mixed Methods International Research Association (MMIRA).

---

## How MMR Works: Integration Process

**Sequential vs. Concurrent Design**  
1. **Sequential** - One method follows the other, building on initial findings.
2. **Concurrent** - Both methods are used simultaneously to gather complementary data.

**Emphasis on Integration**  
- MMR focuses on **merging** data to form a cohesive narrative rather than simply presenting separate analyses.

---

## Applications and Utility of MMR

**Applicability**  
- Particularly valuable for complex social science research questions where neither qualitative nor quantitative alone is sufficient.
- Provides insights that are both deep (qualitative) and broad (quantitative).

**Fields of Application**  
- Education, healthcare, social sciences, and psychology are prime examples where MMR yields comprehensive insights.

---

## Criticisms and Challenges of MMR

**Challenges**  
1. **Epistemological Tension** - Balancing philosophical differences between qualitative and quantitative paradigms.
2. **Methodological Rigidity** - Risk of standardisation that may restrict flexibility.
3. **Complexity of Integration** - Combining data cohesively is challenging.

**Potential Solution**  
- Emphasis on reflexivity and adaptability to each research context rather than strict adherence to typologies.

---

## Conclusion: The Future of Mixed Methods Research

**Future Directions**  
- MMR's growth shows its potential to become a distinct field within social science.
- Increasing demand for complex analysis frameworks suggests MMR will continue to evolve and adapt.

**Final Thought**  
MMR offers a holistic approach to understanding multifaceted social issues by leveraging both qualitative and quantitative insights.

---
class: inverse, center, middle

# Life after SPSS4102/6006

---

## Life after SPSS4102/6006

- Keep engaging with debates and advances in quantitative methods for the social sciences

  - https://www.reddit.com/r/dataisbeautiful/
  - https://www.reddit.com/r/datascience/
  - https://www.reddit.com/r/CompSocial/
  
- Keep engaging with computer programming, R and RStudio

  - https://www.reddit.com/r/rstats/
  - https://www.reddit.com/r/RStudio/
  - https://www.meetup.com/rladies-sydney (Next event 6 Nov!)

---

## Keep a project-oriented approach

1. What do you want to find?

2. What data do you need?

3. What software tools can you use to do it?

## Keep familiarising yourself with what chatbots can do for you

- GenAI has changed how quickly you can become very proficient in a computer language: with ChatGPT, Copilot, Gemina you already are a very sophisticated and knowledgeable programmer.

- But always make sure you are (more or less) in control of what is going on. Don't let the machines take over!

---

## The availability of 

- Data and 
- Data analysis 

tools offers great opportunities for understanding and improving society.

### And yet, with great power comes great responsability ...

.center[&lt;iframe src="https://giphy.com/embed/6rCk8D1VZwm52" width="480" height="350" style="" frameBorder="0" class="giphy-embed" allowFullScreen&gt;&lt;/iframe&gt;]

---

## Data Are About People

Let’s keep these questions in mind:

.large[

- Should this data be collected?

- Are people represented fairly by this data?

- Does the resulting analysis represent these people fairly?

]
  





---
class: inverse, center, middle

# Attendance

---
class: inverse, center, middle

# Let's workshop your A3!


---
class: inverse, center, middle

# Thanks for joining in and engaging during the semester!


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="assets/remark-zoom.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "4:3",
  "navigation": {
    "scroll": false
  }
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
