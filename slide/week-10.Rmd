---
title: "SSPS [4102|6006] Data Analytics</br>[in the Social Sciences|for Social Research]"
subtitle: "Week 10</br>Textual Data: Natural Language Features"
author: "Francesco Bailo"
date: "Semester 1, 2025 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '4:3' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling

---

background-image: url('assets/USydLogo.svg')
background-size: 95%

<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

```{r setup, include=FALSE}

options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, 
                      dev = 'svg', 
                      fig.width = 4, 
                      fig.height = 4, out.width="30%",
                      fig.align="center")

library(knitr)
library(kableExtra)
library(tidyverse)
library(sf)
library(DiagrammeR)
library(cowplot)
library(gapminder)
library(ggrepel)

ggplot2::theme_set(theme_bw())

```

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.
---

## Recap from last week

- Machine learning
- K-nearest neighbors algorithm (k-NN) classification (2+ classes)
- Linear regression (again!) for numeric prediction
- Logistic regression for binary classification (2 classes)


---

## Today's class

| Time        | Content                             |
|-------------|-------------------------------------|
| 14:00-14:30 | Individual on A3 with peer-review   |
| 14:30-15:15 | Individual Problem Set              |
| 15:15-15:30 | Language and Modelling              |
| 15:30-15:45 | Tokenization, Stop Words & Stemming |
| 15:45-15:55 | Individual quiz/tutorial Part 1     |
| 15:55-16:00 | Break                               |
| 16:00-16:15 | Sentiment analysis                  |
| 16:15-16:30 | Document-Frequency Matrix           |
| 16:30-16:40 | Word Embeddings                     |
| 16:40-17:00 | Individual quiz/tutorial Part 2     |

---
class: inverse, center, middle

# Individual on A3 with peer-review

```{r echo = FALSE}
library(countdown)

countdown(minutes = 30, seconds = 00)
```

---
class: inverse, center, middle

# Individual problem set

```{r echo = FALSE}
library(countdown)

countdown(minutes = 45, seconds = 00)
```


---

class: inverse, center, middle

# Textual Data: Natural Language Features


.footnote[Slides adapted from Hvitfeldt, E., & Silge, J. (2021). *Supervised machine learning for text analysis in R*. Chapman and Hall/CRC. https://doi.org/10.1201/9781003093459
]

---
class: inverse, center, middle
# Introduction to these two weeks on Text Analysis

---

## Supervised Modeling for Text Analysis

### Modelling as a Statistical Practice

As seen .content-box-green[*modelling*] can include a wide range of activities; over these two weeks we introduce supervised or predictive modelling **using text data to make predictions about the world**.

### Types of Models (next week)

1. **Regression Models**: Predict numeric or continuous outcomes (e.g., predicting the year of a U.S. Supreme Court opinion based on its text).
2. **Classification Models**: Predict discrete outcomes or class labels (e.g., classifying news articles to study media bias).

---

## Supervised Modeling for Text Analysis

###  Importance of Text Data

Text data is vital across fields—from healthcare to digital humanities and social sciences However, specialised methods are essential to transform natural language into a machine-readable format.

### Text Preprocessing (this week)

*This week* we explore typical preprocessing steps from scratch making language data ready for computation and modelling (which we will cover *next week*).


---
class: inverse, center, middle

# Language and modeling

---

## Introduction

- **Text Analysis and Linguistics**: Machine learning and deep learning models are powerful tools for text analysis, but they are fundamentally shaped by human understanding of language.

- **Bridging the Gap**: Data scientists, often without formal training in linguistics, can benefit greatly from understanding how language works to build more effective text analysis models.

---

## The Role of Linguistics in Text-Based Machine Learning

### Why Linguistics Matters:

- Language is complex, ambiguous, and contextual, impacting the reliability of machine learning models.

  - "Include Your Children When Baking Cookies"

- By leveraging linguistic knowledge, we **create better features** that allow models to handle nuances in text data.

**Example**: Understanding sentence structures or word formations can lead to more accurate text representations in models.

---

## A text analysis pipeline

```{r echo = F, out.width='100%'}
library(DiagrammeR)

# Create the flowchart
grViz("
digraph text_analysis_workflow {
  
  # Define graph attributes
  graph [layout = dot, rankdir = LR]
  
  # Define node styles and labels
  node [shape = box, style = filled, color = lightgrey]
  
  text_data      [label = 'Text Data']
  preprocessing  [label = 'Preprocessing:\nCleaning,\nTokenization,\nStemming,\nRemoving \nStop words']
  feature_extraction [label = 'Feature Extraction']
  model          [label = 'Model']
  classification  [label = 'Classification']
  prediction     [label = 'Prediction']
  
  # Define edges to show the flow
  text_data -> preprocessing
  preprocessing -> feature_extraction
  feature_extraction -> model
  model -> classification
  model -> prediction
}
")
```


---

## Key Linguistic Subfields Relevant to Text Analysis

Text processing benefits from understanding different linguistic levels, from sounds to sentence structure.

| Subfield      | Focus                                 | Example Usage in Text Analysis               |
|---------------|---------------------------------------|----------------------------------------------|
| Phonetics     | Sounds used in language               | N/A for text (but relevant for speech data)  |
| Morphology    | Structure and formation of words      | Stemming, lemmatization                      |
| Syntax        | Formation of sentences                | Part-of-speech tagging, dependency parsing   |
| Semantics     | Meaning of words and sentences        | Sentiment analysis                           |
| Pragmatics    | Contextual language use               | Disambiguation, context analysis             |

---

## Morphology - Transforming Words for Analysis


.content-box-yellow[

**Morphology** studies the structure of words, including morphemes, which are the smallest units of meaning.

]

Different languages vary in their use of morphemes, impacting model performance based on language-specific processing needs.

**Example**: English has fewer morphemes per word, while languages like Turkish and Russian have more, influencing how we preprocess text for each.

---

## Morphological Processing in R - Tokenization and Stemming (more on this later)

#### Step 1

```{r}
library(tidytext)
library(SnowballC)

# Sample text
text <- 
  c("Text analysis in machine learning can benefit from morphological processing.")
text_df <- 
  data.frame(line = 1, text = text)
text_df
```

---

#### Step 2

```{r}
# Tokenize text into individual words
tokens <- text_df |>
  tidytext::unnest_tokens(word, text)
tokens
```

---

#### Step 3

```{r}
# Apply stemming to reduce words to their root form
tokens |>
  mutate(stem = SnowballC::wordStem(word))
```

---

## Morphological Processing in R - Tokenization and Stemming (more on this later)

1. **Tokenization**: Breaks down text into individual words (tokens).

2. **Stemming**: Reduces words to their base or *root* form, standardising variations (e.g., "running" to "run"). So words with the same *root* are comparable.


---

## Handling Language Diversity in Machine Learning Models

.content-box-red[

Machine learning models often fail to generalize across different languages or dialects.

]

- For example, a model trained on English may struggle with dialects like African American Vernacular English (AAVE), falsely identifying benign terms as harmful.

**Best Practice**:

- Explicitly state the language in model documentation.

- If working with multiple dialects, ensure the model includes training data representative of each.

---

## Text Context and Domain-Specific Language

- Text varies greatly by context; models trained on one type (e.g., tweets) may perform poorly on another (e.g., medical texts).

- For domain-specific tasks, tailored training data is crucial for accurate predictions.

**Example**: A sentiment model trained on social media text may misinterpret neutral medical terms as negative if applied to clinical documents.

**Best Practice**: Use context-specific text data for training to avoid bias and improve model performance.

---

## Limitations of Text-Based Models


- Text data cannot fully capture the **high-dimensional nature of language**, leading to inherent model limitations.

- Modelling assumes simplifications, which may omit crucial context or meaning.

**Implications**:

1. Accept that machine learning models have limitations based on the training data and the linguistic features used.

2. Regularly update and retrain models with recent, contextually relevant data to maintain accuracy.

---
class: inverse, center, middle

# Tokenization

---

## Tokenization - Turning Text into Tokens

**What for**? To Transform raw text into structured tokens to create features for machine learning.

### Overview

- Tokenization splits text into meaningful units (tokens) for computational analysis.

- Tokens can be words, characters, n-grams, and more.

---

## What is a Token?

.content-box-yellow[**Definition**: A token is a *unit of text*, often a word, used in analysis.]

**Examples**:

- **Character Tokens**: Individual letters (e.g., "m", "a", "c").
- **Word Tokens**: Single words (e.g., "machine").
- **Sentence Tokens**: Single sentence (e.g., "This machine is broken.").
- **Paragraph Tokens**: Single paragraph.
- **N-grams**: Sequences of words or characters.


---

## Tokenizing Text Data

**Objective**: Break down text for analysis using tokens.

**Example**: Let's use Mary Shelley's Frankestein for the analysis.

```{r eval = FALSE}
install.packages("gutenbergr")
```

Note: gutenbergr download books into data frames where each row is a book's line.

```{r}
library(gutenbergr)
frankenstein <- 
  gutenberg_works(title == "Frankenstein; Or, The Modern Prometheus") |>
  gutenberg_download()
```

---

## Word Tokenization

**Purpose**: Split text into individual words.

We use the function `tokenize_words` from the *tokenizers* package to split the 49th line of the book.

```{r eval = FALSE}
install.packages("tokenizers")
```


```{r}
frankenstein$text[49]
```


```{r}
library(tokenizers)
word_tokens <- tokenizers::tokenize_words(frankenstein$text[49])
word_tokens
```

---

## Tokenizing with tidytext

**Tidytext Workflow**: Convert text into tidy data for seamless analysis with dplyr.

.content-box-green[

Remember that tidy data is a data frame where each row is one observation (or unit of analysis) with its value.

]

```{r eval = FALSE}
install.packages("tidytext")
```

.small[

.pull-left[
In this example a row or unit of analysis is a single word.
]

.pull-right[
.center[

| line | word   |
|------|--------|
| 1    | machine|
| 1    | learning|
| 1    | in     |
| 1    | text   |
| 1    | analysis|
| 1    | is     |
| 1    | exciting|

]

]
]

---

## The three rules of tidy data

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

<img src = 'https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png'></img>

.footnote[Wickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (2nd edition). O’Reilly Media, Inc. https://r4ds.had.co.nz/tidy-data.html]

---
.small[
The `unnest_tokens` function takes a tidy data frame where each row contains some text, here `text`, in a dedicated column and additional information (i.e., metadata), here `book` with the title of the book and return another tidy data frame where the text column is replaced by the tokenised text (in this case using "words").  
]

```{r}
library(tidytext)
frankenstein |> # From before
  tidytext::unnest_tokens(output = word, # Output column name
                          input = text, # Input column with text
                          token = "words") # Tokenize what?
```

---

## Additional preprocessing of text

We can probably already do some cleaning, by removing all the lines that are not properly part of the book using the dplyr function `slice`.

```{r}
frankenstein <-
  frankenstein |>
  dplyr::slice(46:dplyr::n()) 
  # this will only include rows from 46 to the last row.
  # which we get with the function `n()`
```


.content-box-red[

Preprocessing your text is critical. Text often comes with textual information you don't want to include because is not really part of the document.

]

- Preprocessing might also include set all characters to lower-case (this is done by the `unnest_tokens` function)

---

And after some cleaning we tokenize again but this time we use n-grams

```{r}
library(tidytext)
frankenstein |> # From before
  tidytext::unnest_tokens(output = word, # Output column name
                          input = text, # Input column with text
                          token = "ngrams", # Tokenize what?
                          n = 3) # How many grams?
```

---

## Difference between `token = "words"` and `token = "ngrams"`

.small[

- Note that the word "preface", which now appears on the first row of the data frame `frankenstein` doesn't appear when we use tri-grams as is single word in its line. 

.pull-left[

#### Words

```{r echo = FALSE}
library(tidytext)
frankenstein |> # From before
  tidytext::unnest_tokens(output = word, # Output column name
                          input = text, # Input column with text
                          token = "words") 
```

]

.pull-right[

#### Trigrams (3-grams)

```{r echo = FALSE}
library(tidytext)
frankenstein |> # From before
  tidytext::unnest_tokens(output = word, # Output column name
                          input = text, # Input column with text
                          token = "ngrams", # Tokenize what?
                          n = 3) # How many grams?
```

]

]

---

## Understanding N-grams

.small[

**Definition**: N-grams are sequences of $n$ words that capture phrase-level patterns.

Note that words in bi-grams and tri-grams overlap. 

```{r}
frankenstein$text[4]
tokenize_ngrams(frankenstein$text[4], n = 2)
tokenize_ngrams(frankenstein$text[4], n = 3)
```

]

---
class: inverse, center, middle

# Stop Words

---

## Introduction to Stop Words

### What are Stop Words?

- Common words in a language (e.g., "the," "and") that **carry little meaning** in NLP tasks.
- Removing stop words can simplify text data, improving model efficiency.

### History

- Term coined by Hans Peter Luhn in 1960.
- Initially used to reduce computation time in text mining.
- Yet, Stop Words might play an important role in specific text, for example in *stylometry*, which is used to predict the authorship of documents (e.g., finding that the author of The Cuckoo’s Calling was in fact J.K. Rowling).    

---

## Treating text as a collection of features

<iframe width="560" height="315" src="https://www.youtube.com/embed/KS1xrYfGWuA?si=DKan_RMzt0F8i-JA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

---

## Categories of Stop Words

### Types of Stop Words:

- **Global Stop Words**: Commonly uninformative words across many texts (e.g., "of," "in").
- **Subject-Specific Stop Words**: Words that lack meaning in specific contexts (e.g., "bedroom" in real estate listings).
- **Document Stop Words**: Low-information words unique to individual documents.

---

## Using Pre-Made Stop Word Lists

.small[

- Premade lists like SMART, Snowball, and onix contain common stop words.
- Different lists have varying lengths and words based on their intended use.

```{r eval = F}
data(stop_words) # load the data from the tidytext package
```

You can access "standard" lists of Stop Words using the `stop_words` data frame.

.pull-left[

```{r}
head(stop_words)
```

]

.pull-right[

```{r}
table(stop_words$lexicon)
```

]




]

---

## Removing Stop Words with R


```{r}
library(dplyr)
library(tidytext)
text_data <- data.frame(text = "The quick brown fox jumps over the lazy dog")
text_data <- text_data |> tidytext::unnest_tokens(word, text)

# Remove stop words using Snowball list
clean_text <- 
  text_data |>
  dplyr::anti_join(stop_words) # return all rows from x without a match in y.
print(clean_text)
```


---

## More on joining tables with dplyr

.center[<img src = "https://tavareshugo.github.io/r-intro-tidyverse-gapminder/fig/07-dplyr_joins.svg"></img>]


---

## Creating Custom Stop Word Lists

### Why Customise?

.small[

Premade lists may not fit your dataset's unique requirements.

**Steps to Customise**:

1. Identify high-frequency words that add no value to your model.
2. Tailor the list to your domain (e.g., removing all animal names).

```{r}
# Custom stop words for real estate
custom_stopwords <- data.frame(word = "fox", "dog") # col name is important!
filtered_data <- text_data |> dplyr::anti_join(custom_stopwords)
print(filtered_data)
```

]

---

## Considerations and Limitations

**Key Takeaways**:

- Stop word removal reduces data complexity but may lose context-specific information.
- Always review premade stop word lists for accuracy and bias.
- Experiment with different stop word lists and document the impact on model performance (and results).

---
class: inverse, center, middle

# Stemming

---

## Introduction to Stemming

### What is Stemming?

- Stemming reduces words to their base or root form, known as the "stem."
= Helps to group variations of words, such as "tree" and "trees."

.content-box-yellow[

**Purpose**: Simplifies text by reducing different forms of a word, decreasing the feature space.

]

(Remember that quantitative text analysis is all about bringing your texts into a **feature space** that you define).

---

## The Porter Stemming Algorithm

- Most widely-used stemming algorithm for English, available via the *SnowballC* package in R.
- Rules-based algorithm that systematically reduces words (e.g., "stories" becomes "stori").

```{r}
library(SnowballC)
SnowballC::wordStem(c("win", "winning", "winner"))
```


---

## Chaining tokenisation, stopwords and stemming in the same pipeline

```{r}
frankenstein_tokens <- 
  frankenstein |>
  # Tokenization
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  # Stemming
  dplyr::mutate(stemmed_word = SnowballC::wordStem(word))

```

---
```{r}
head(frankenstein_tokens)
```

---

## Should You Use Stemming?

- Stemming can reduce data sparsity but may discard important details.
- Not always beneficial for certain text models, as it may overgeneralise.
- In topic modeling, aggressive stemming can reduce coherence and topic stability.


---

## Difference Between Stemming and Lemmatisation

### Stemming:

- Reduces words to their base form without regard to meaning or grammatical role.
- **Example**: "stories" -> "stori"

### Lemmatization:

- Converts words to their dictionary form (lemma) based on context.
- Requires linguistic knowledge (e.g., part of speech) for accuracy.
- **Example**: "running" -> "run"

---

class: inverse, center, middle

# Individual quiz/tutorial Part 1 
(to Stemming included)

```{r echo = FALSE}
library(countdown)

countdown(minutes = 10, seconds = 00)
```

---
class: inverse, center, middle

# Sentiment analysis


---

We already have the tools to conduct some simple sentiment analysis. 

## Sentiment analysis

### What is Sentiment Analysis?

- A technique to determine the emotional tone (e.g., positive, negative, neutral) of text data.
- Useful in understanding public opinion, customer feedback, social media posts, etc.

.content-box-yellow[
Sentiment analysis is about associating the words of a document with a **dictionary** where words are described in terms of sentiments, either on a continuous scale (from negative to positive) or with categories.
]

---

## Types of lexicons (or dictionaries) for sentiment analysis 

.small[

Let's downland four lexicons available with the tidytext package

```{r}
sent_bing <- tidytext::get_sentiments(lexicon = "bing")
sent_afinn <- tidytext::get_sentiments(lexicon = "afinn")
sent_loughran <- tidytext::get_sentiments(lexicon = "loughran")
sent_nrc <- tidytext::get_sentiments(lexicon = "nrc")
```

```{r}
nrow(sent_bing)
nrow(sent_afinn)
nrow(sent_loughran)
nrow(sent_nrc)
```

]
---

```{r}
sent_bing %>%
  dplyr::sample_n(15)
```

---

```{r}
sent_afinn %>%
  dplyr::sample_n(15)
```

---

```{r}
sent_loughran %>%
  dplyr::sample_n(15)
```

---

```{r}
sent_nrc %>%
  dplyr::sample_n(15)
```

---

## Let's compare the sentiment in the 2016 US presidential debate

Let's load the data first...

```{r}
load("../data/debate_transcripts.rda")
```

```{r}
head(debate_transcripts)
```

---

## Donald Trump sentiment in 2016 (AFINN)

```{r}
trump_2016_afinn <- 
  debate_transcripts |>
  dplyr::filter(speaker == "Donald Trump", election_year == 2016) |>
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  # Add sentiment
  dplyr::inner_join(sent_afinn)
trump_2016_afinn
```

---

## Hillary Clinton sentiment in 2016 (AFINN)

```{r}
clinton_2016_afinn <- 
  debate_transcripts |>
  dplyr::filter(speaker == "Hillary Clinton", election_year == 2016) |>
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  # Add sentiment
  dplyr::inner_join(sent_afinn)
clinton_2016_afinn
```

---

## Who's more using more negative terms?

```{r}
median(trump_2016_afinn$value)
mean(trump_2016_afinn$value)
```

```{r}
median(clinton_2016_afinn$value)
mean(clinton_2016_afinn$value)
```

---

## Donald Trump sentiment in 2016 (NRC)

```{r}
trump_2016_nrc <- 
  debate_transcripts |>
  dplyr::filter(speaker == "Donald Trump", election_year == 2016) |>
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  # Add sentiment
  dplyr::inner_join(sent_nrc)
trump_2016_nrc
```

---

## Hillary Clinton sentiment in 2016 (NRC)

```{r}
clinton_2016_nrc <- 
  debate_transcripts |>
  dplyr::filter(speaker == "Hillary Clinton", election_year == 2016) |>
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  # Add sentiment
  dplyr::inner_join(sent_nrc)
clinton_2016_nrc
```

---

.pull-left[

```{r out.width = "100%"}
trump_2016_nrc |>
  ggplot(aes(x = sentiment)) +
  geom_bar() +
  scale_x_discrete(guide = guide_axis(angle = 45))
```

]

.pull-right[

```{r out.width = "100%"}
clinton_2016_nrc |>
  ggplot(aes(x = sentiment)) +
  geom_bar() +
  scale_x_discrete(guide = guide_axis(angle = 45))
```

]



---
class: inverse, center, middle

# The Document-Frequency Matrix

---

## The Document-Frequency Matrix

Let's say you have six *documents* you want to analyse. These six documents constitute your **collection** (a.k.a. **corpus**).

```{r}
documents <- c(
  "Political parties influence policy decisions",
  "Elections determine the political landscape",
  "Governments implement policies for public welfare",
  "Economic growth depends on consumer spending",
  "Inflation affects purchasing power in the economy",
  "Fiscal policy aims to stabilize economic fluctuations"
)
```

--

- **Problem**: you must **quantify** these documents **within the same space** so that you can analyse them together.

- **Solution**: You can **count the frequency of each word** in each document (and create a document-frequency matrix, or DFM)!

---

## The Document-Frequency Matrix (DFM) in R

Step 1. Our documents into a tidy data frame

```{r}
text_data <- 
  data.frame(doc_id = 1:6, text = documents) # Our six documents are now in a df
```

Step 2. Let's count the **frequency of each word in each document**

```{r}
my_word_frequency <- 
  text_data %>%
  tidytext::unnest_tokens(word, text) |> # 1. Tokenise the documents
  dplyr::anti_join(stop_words) |> # 2. Remove Stop Words
  dplyr::count(doc_id, word)
head(my_word_frequency)
```

---

Step 2-b Let's count the **frequency of each word in Trump and Clinton**

```{r}
trump_clinton_2016_freq <-
  debate_transcripts |>
  dplyr::filter(speaker %in% c("Donald Trump",
                               "Hillary Clinton"), 
                election_year == 2016) |>
  tidytext::unnest_tokens(output = word, input = text, 
                          token = "words", to_lower = TRUE) |>
  # Removing stop words
  dplyr::anti_join(stop_words) |>
  dplyr::count(speaker, word)
head(trump_clinton_2016_freq)
```

---

```{r out.width = "70%", fig.width=4, fig.width=6}
trump_clinton_2016_freq |>
  group_by(speaker) |>
  slice_max(n, n = 15) |>
  ungroup() |>
  ggplot(aes(n, fct_reorder(word, n), fill = speaker)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~speaker, ncol = 2, scales = "free") +
  labs(x = "frequency", y = NULL)
```



---

Step 3. Let's create the document-term matrix

.small[

```{r}
my_dfm <- 
  my_word_frequency |>
  # This requries install.packages('quanteda')
  tidytext::cast_dfm(document = doc_id, # Column identifying the document
                     term = word, # Column identifying the term
                     value = n) # Column identifying the frequency value
print(my_dfm)
```

DFM is commonly used but also inefficient because **sparse** (it's mostly zeros!).

]

---

## Term Frequency-Inverse Document Frequency

So far we have created a document-frequency matrix using the simple frequency of words in a document. If a word (or term) appears 4 times in a document, the reported value will be 4. 

Yet it is probably possible to do better than that especially if we want to identify and weight up words that are more important to set documents apart.

- Think about words that only appears in one or two documents, and non in the rest of the corpus.

**Definition**: TF-IDF stands for .content-box-yellow[Term Frequency-Inverse Document Frequency], a statistical measure used to evaluate the importance of a word in a document relative to a collection (corpus) of documents.

**Purpose**: Highlights words that are unique or distinctive to specific documents while down-weighting common words (e.g., "the," "and").

---

## Understanding Term Frequency (TF) and Inverse Document Frequency (IDF)

- **Term Frequency (TF)**: Measures how often a word appears in a document.

$$TF = \frac{\text{Total terms in document}}{Number of times term appears in document}$$

- **Inverse Document Frequency (IDF)**: Reduces the weight of terms that frequently appear across many documents.

$$IDF = log \left( \frac{\text{Total number of documents}}{\text{Number of documents containing term}} \right) $$

 
---

## Calculating TF-IDF

$$TF-IDF=TF\timesIDF$$

### Interpretation:

- **High TF-IDF scores**: Words that are unique to a particular document and carry more meaning within that context.
- **Low TF-IDF scores**: Common words across the corpus, often less meaningful for distinguishing documents.

### Example:

Words like "algorithm" in a tech-focused document corpus will have higher TF-IDF scores than words like "the" or "is."

---

## TF-IDF in R

To calculate TF-IDF you can use `bind_tf_idf()` from the tidytext package

```{r}
my_word_tfidf <- 
  text_data |>
  tidytext::unnest_tokens(word, text) |> # 1. Tokenise the documents
  dplyr::filter(!word %in% stopwords::stopwords()) |> # 2. Remove Stop Words
  dplyr::count(doc_id, word) |> # 3. Count the frequency
  tidytext::bind_tf_idf(word, doc_id, n) # 4. Calculate and Bind TF-IDF column
head(my_word_tfidf)
```

---

You can then create your document-term matrix with the same function seen above (`tidytext::cast_dfm()`).

```{r}
my_word_tfidf |>
    tidytext::cast_dfm(document = doc_id, # Column identifying the document
                     term = word, # Column identifying the term
                     value = tf_idf) # Column identifying the frequency value
```

---

- **Problem**: Sparse matrices are memory-intensive and unsuitable for capturing context. Instead...
- **Solution**: Word embeddings capture semantics by learning word relationships (more efficiently than DFM).

> "Modern word embeddings are based on a statistical approach to modeling language, rather than a linguistics or rules-based approach."

---
class: inverse, center, middle

# Word Embeddings

---

## What are embeddings?

.content-box-yellow[
**Embeddings** are a fundamental tool that bridges the gap between *qualitative* and *quantitative* data—especially useful in social sciences where important data is often textual or categorical (or visual).
]

- At their core, embeddings are **numerical representations** of data elements, such as words, sentences, documents, or even users, in a **continuous vector space**. 

- This means we **convert qualitative data into a series of numbers** (vectors) that a computer can process mathematically.

- For **Text Data**: Words or phrases are transformed into vectors of numbers.
- For **Categorical Data**: Categories or classes are represented in a numerical format that *preserves their relationships*.

---

## Why Do We Need Embeddings?

In data analytics, especially with machine learning algorithms, we need numerical inputs. Embeddings allow us to:

- **Capture Semantic Meaning**: Words with similar meanings have vectors that are close together in the embedding space.
- **Perform Mathematical Operations**: Enable calculations like measuring similarity or clustering data points.
- **Reduce Dimensionality**: Compress high-dimensional data (with a too many features) into lower-dimensional space without losing significant information.

---
## How Do Embeddings Work?

.small[

#### 1. Representing Data in Vector Space

- Imagine a multi-dimensional space where each dimension represents a **feature** of the data.

- Each data element is a point in this space, defined by its **vector coordinates**.

#### 2. Training Embeddings

- Embeddings are learned from data using neural networks or other machine learning models.

- Example: In natural language processing (NLP), models like Word2Vec are trained on large text corpora to learn word embeddings based on context.

#### 3. Contextual Relationships

- The position of each vector reflects the relationships between data elements.

- **Semantic Proximity**: Words that appear in similar contexts are located near each other in the vector space.

]
---

## Analogies to Understand Embeddings

.content-box-yellow[

###Think of Embeddings Like a Map

- Cities and Distances: Each city is a point on a map with geographic coordinates. The distance between cities tells you how close they are geographically - if they are really close they might be part of the same community (e.g., state or nation).

]


---

## Introduction to Word Embeddings

.content-box-yellow[

**Definition**: Word embeddings are dense vector representations of words that capture semantic meaning based on context.

]

> "You shall know a word by the company it keeps." — John Rupert Firth

## But why do need word embeddings?

Traditional text representations are sparse and high-dimensional. Word embeddings provide a dense, low-dimensional alternative that captures semantic similarity.

---

## Creating Word Embeddings – An Overview

Process: Word embeddings are learned by analysing the co-occurrence of words in a large text corpus.

- **Step 1**: Process the corpus and identify word contexts (e.g., via sliding windows).
- **Step 2**: Measure word association using statistical methods, such as Pointwise Mutual Information (PMI).
- **Step 3**: Apply dimensionality reduction techniques (e.g., Singular Value Decomposition, SVD) to create embeddings.

---

## The Concept of Context – The Role of Windows

### What is a Sliding Window?

- A **window** is a defined span of text around each word, allowing the model to "see" a word's immediate neighbours.

**Example**: In "The cat sat on the mat," a window size of 2 around "sat" includes ["cat", "on"].

**Purpose**: Windows capture the local context around each word, encoding relationships that influence semantic meaning.

---

## How Windows Affect Word Embeddings

### Window Size Implications:

- **Small Windows** (e.g., 2-3 words): Capture *functional similarity*, such as words with similar grammatical roles (e.g., verbs near "run").
- **Large Windows** (e.g., 10+ words): Capture **topical similarity**, meaning words related to the same theme or subject.
**Key Insight**: Window size influences the type of relationship captured by embeddings, from grammatical to semantic.

---

## Creating Custom Embeddings – Window-Based Context

### How It Works:

- For each word in a text corpus, the model looks within a defined window to learn which words commonly co-occur.
- Word pairs found within these windows influence the final vector representation of each word.

### Why This Matters:

**Windows help embeddings differentiate between words** with different contexts or meanings (e.g., "bank" in finance vs. nature).

---

## Word Co-Occurrence – Context in Action

### Pointwise Mutual Information (PMI):

- A statistical measure used to calculate how often two words appear together within a window.
- Higher PMI values indicate strong associations (e.g., "coffee" and "cup").

**Result**: Words with high co-occurrence within windows gain similar vector representations, capturing context-specific meaning.

---

## Pre-Trained Word Embeddings – An Overview

### What Are They?

- Pre-trained embeddings are word vectors learned from large, publicly available corpora like Wikipedia or news articles.

- Examples include GloVe, Word2Vec, and FastText.

---

## Use Pre-Trained Word Embeddings in R

Refer to the text book for more details, but note that Pre-Trained models can be very large (GB large).

For example, 

- the `embedding_glove6b()` function of the textdata package will download a 6 billion tokens model (size: 822.2 MB)

- the `embedding_glove840b()` function will download a 840 billion token models 4.03 GB

Once downloaded, you can use the vectors for the words of your corpus for your analysis (instead of calculating these vectors yourself)

---
class: inverse, center, middle

# Individual quiz/tutorial Part 2

```{r echo = FALSE}
library(countdown)

countdown(minutes = 20, seconds = 00)
```

---
class: inverse, center, middle

# Attendance

---
class: inverse, center, middle

# See you next week for the second week on Text Analysis!


